[["index.html", "GOVT 8001: Shared Working Book 1 Introduction 1.1 Purpose of This Site 1.2 Loading Data", " GOVT 8001: Shared Working Book Benjamin Reese 1 Introduction 1.1 Purpose of This Site The goal of this site is to have a place where you can look for examples of code. Think of this site as a log of all of the topics we cover in class and in lab sessions. I will make updates throughout the semester as we cover more advanced material. Let me know if any of this code doesn’t run correctly or if you have any questions or issues! 1.2 Loading Data The easiest way to load data into R and ensure you have the correct file path is to create a folder on your computer for each assignment and place the datasets directly into that folder. Create a folder on your computer for each new analysis Download your Data and move the file to your newly created folder Then open RStudio Click the project button in the top right corner Click new project Click existing directory Click browse and find the folder that you created Click create project Once your new project opens, click the blank page with a green plus sign icon in the top left corner under the file option Click R script to open a new script You should also be able to see your data file in the bottom right window of RStudio, click the file and follow the options depending on the file type Once your data is imported into R, the code that R automatically ran will be in the console window on the bottom left, copy and paste it to your fresh R script For example, in Lab 1, my code looked like: read_excel(\"Data/USstates.xlsx\") Run this copy and pasted line of code whenever you open the R Project and you will never have to worry about complicated file pathing commands I recommend using the assignment operator &lt;- to give your dataset a short and simple name like df, dta, or, if you are working with multiple datasets, name each something short and descriptive. "],["lab-i-introduction-to-r-r-studio.html", "2 Lab I: Introduction to R &amp; R Studio 2.1 Intro to R 2.2 Intro to library(tidyverse)", " 2 Lab I: Introduction to R &amp; R Studio 2.1 Intro to R # Author: QSS Ch. 1 script with edits by Mark Richardson &amp; Benjamin Reese # Date: 08/24/2023 # Purpose: Introduction to R - GOVT 8001 Lab I #### Arithmetic Operations #### 5 + 3 ## [1] 8 5 - 3 ## [1] 2 5 / 3 ## [1] 1.666667 5 ^ 3 ## [1] 125 5 * (10 - 3) ## [1] 35 sqrt(4) ## [1] 2 #### Objects #### result &lt;- 5 + 3 result ## [1] 8 print(result) ## [1] 8 result &lt;- 5 - 3 result ## [1] 2 ## R is case sensitive so we get an error. Result kosuke &lt;- &quot;instructor&quot; kosuke ## [1] &quot;instructor&quot; kosuke &lt;- &quot;instructor and author&quot; kosuke ## [1] &quot;instructor and author&quot; Result &lt;- 5 Result + 2 ## [1] 7 result ## [1] 2 class(result) ## [1] &quot;numeric&quot; Result ## [1] 5 class(Result) ## [1] &quot;numeric&quot; class(sqrt) ## [1] &quot;function&quot; sum(result) ## [1] 2 sum(Result) ## [1] 5 #### Vectors #### # Creating vectors world.pop &lt;- c(2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183) world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 pop.first &lt;- c(2525779, 3026003, 3691173) pop.second &lt;- c(4449049, 5320817, 6127700, 6916183) pop.all &lt;- c(pop.first, pop.second) pop.all ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 # Accessing elements of a vector world.pop[2] ## [1] 3026003 world.pop[c(2, 4)] ## [1] 3026003 4449049 world.pop[c(4, 2)] ## [1] 4449049 3026003 world.pop[-3] ## [1] 2525779 3026003 4449049 5320817 6127700 6916183 # Arithmetic operations on a vector pop.million &lt;- world.pop / 1000 pop.million ## [1] 2525.779 3026.003 3691.173 4449.049 5320.817 6127.700 6916.183 pop.rate &lt;- world.pop / world.pop[1] pop.rate ## [1] 1.000000 1.198047 1.461400 1.761456 2.106604 2.426063 2.738238 pop.increase &lt;- world.pop[-1] - world.pop[-7] pop.increase ## [1] 500224 665170 757876 871768 806883 788483 percent.increase &lt;- (pop.increase / world.pop[-7]) * 100 percent.increase ## [1] 19.80474 21.98180 20.53212 19.59448 15.16464 12.86752 # Can replace individual elements (better way is to use round()) round(percent.increase) ## [1] 20 22 21 20 15 13 percent.increase[c(1, 2)] &lt;- c(20, 22) percent.increase ## [1] 20.00000 22.00000 20.53212 19.59448 15.16464 12.86752 #### Functions #### length(world.pop) ## [1] 7 min(world.pop) ## [1] 2525779 max(world.pop) ## [1] 6916183 range(world.pop) ## [1] 2525779 6916183 mean(world.pop) ## [1] 4579529 sum(world.pop) / length(world.pop) ## [1] 4579529 year &lt;- seq(from = 1950, to = 2010, by = 10) year ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(to = 2010, by = 10, from = 1950) ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(1950, 2010, 10) ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(2010, 1950, -10) ## [1] 2010 2000 1990 1980 1970 1960 1950 seq(from = 2010, to = 1950, by = -10) ## [1] 2010 2000 1990 1980 1970 1960 1950 2008:2012 ## [1] 2008 2009 2010 2011 2012 2012:2008 ## [1] 2012 2011 2010 2009 2008 names(world.pop) ## NULL names(world.pop) &lt;- year names(world.pop) ## [1] &quot;1950&quot; &quot;1960&quot; &quot;1970&quot; &quot;1980&quot; &quot;1990&quot; &quot;2000&quot; &quot;2010&quot; world.pop ## 1950 1960 1970 1980 1990 2000 2010 ## 2525779 3026003 3691173 4449049 5320817 6127700 6916183 #### Saving data and loading data #### # Create a data set (Table 1.2) # tibble() is the equivalent of data.frame() tidyverse function from the tibble package UNpop &lt;- data.frame(world.pop = world.pop, year = year) # Get basic information about the data set names(UNpop) ## [1] &quot;world.pop&quot; &quot;year&quot; nrow(UNpop) ## [1] 7 ncol(UNpop) ## [1] 2 dim(UNpop) ## [1] 7 2 summary(UNpop) ## world.pop year ## Min. :2525779 Min. :1950 ## 1st Qu.:3358588 1st Qu.:1965 ## Median :4449049 Median :1980 ## Mean :4579529 Mean :1980 ## 3rd Qu.:5724258 3rd Qu.:1995 ## Max. :6916183 Max. :2010 UNpop$world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 UNpop[, &quot;world.pop&quot;] # extract the column called &quot;world.pop&quot; ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 UNpop[c(1, 2, 3, 5), ] # extract the first three rows (and all columns) ## world.pop year ## 1950 2525779 1950 ## 1960 3026003 1960 ## 1970 3691173 1970 ## 1990 5320817 1990 UNpop[1:3, &quot;year&quot;] # extract the first three rows of the &quot;year&quot; column ## [1] 1950 1960 1970 UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] ## [1] 2525779 3691173 5320817 6916183 # File paths and working directory getwd() # Confirm the change ## [1] &quot;C:/Users/17176/Documents/GOVT701&quot; #### Getting Help: mean() example #### world.pop &lt;- c(UNpop$world.pop, NA) world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 NA mean(world.pop) ## [1] NA ## Use Question Marks to see help documentation ?mean mean(world.pop, na.rm = TRUE) ## [1] 4579529 2.2 Intro to library(tidyverse) # Packages ## install.packages(&quot;devtools&quot;) # install the package library(devtools) # load the package ## install a package from github ## devtools::install_github(&quot;kosukeimai/qss-package&quot;, build_vignettes = TRUE) library(qss) ## loading in qss ## You may need to allow R to update/install additional packages ## Loading in tidyverse ## install.packages(&quot;tidyverse&quot;) library(tidyverse) ## Loading in a Dataset data(UNpop, package = &quot;qss&quot;) ## Number of Rows and Columns - Base R dim(UNpop) ## [1] 7 2 ## Number of observation, number of variables, and initial observations - tidyverse glimpse(UNpop) ## Rows: 7 ## Columns: 2 ## $ year &lt;int&gt; 1950, 1960, 1970, 1980, 1990, 2000, 2010 ## $ world.pop &lt;int&gt; 2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183 ## First 6 rows head(UNpop) ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## 4 1980 4449049 ## 5 1990 5320817 ## 6 2000 6127700 ## Last 6 Rows tail(UNpop) ## year world.pop ## 2 1960 3026003 ## 3 1970 3691173 ## 4 1980 4449049 ## 5 1990 5320817 ## 6 2000 6127700 ## 7 2010 6916183 ## Selecting A Variable - Base R UNpop$world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 ## subset all rows for the column called &quot;world.pop&quot; from the UNpop data UNpop[, &quot;world.pop&quot;] ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 ## subset the first three rows (and all columns) UNpop[c(1, 2, 3),] ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## subset the first three rows of the &quot;year&quot; column UNpop[1:3, &quot;year&quot;] ## [1] 1950 1960 1970 ## Now with tidyverse ## Subset the first three rows of UNpop with tidyverse slice(UNpop, 1:3) ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## Extract/subset the world.pop variable (column) select(UNpop, world.pop) ## world.pop ## 1 2525779 ## 2 3026003 ## 3 3691173 ## 4 4449049 ## 5 5320817 ## 6 6127700 ## 7 6916183 ## Base R subset the first three rows of the year variable UNpop[1:3, &quot;year&quot;] ## [1] 1950 1960 1970 ## or in tidyverse, combining slice() and select() select(slice(UNpop, 1:3), year) ## year ## 1 1950 ## 2 1960 ## 3 1970 ## Basic Data Wrangling with the tidyverse using pipes (i.e., %&gt;%) UNpop %&gt;% # take the UNpop data we have loaded, and then... slice(1:3) %&gt;% # subset the first three rows, and then... select(year) # subset the year column ## year ## 1 1950 ## 2 1960 ## 3 1970 UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% # using a sequence from 1 to n() select(world.pop) ## world.pop ## 1 2525779 ## 2 3691173 ## 3 5320817 ## 4 6916183 pop.1970 &lt;- UNpop %&gt;% # take the UNpop data and then.... filter(year == 1970) %&gt;% # subset rows where the year variable is equal to 1970 select(world.pop) %&gt;% # subset just the world.pop column pull() # return a vector, not a tibble ## Print the vector to the console to see it print(pop.1970) ## [1] 3691173 UNpop.mill &lt;- UNpop %&gt;% # create a new tibble from UNpop mutate(world.pop.mill = world.pop / 1000) %&gt;% # create a new variable, world.pop.mill select(-world.pop) # drop the original world.pop column ## Adding a variable with if_else UNpop.mill &lt;- UNpop.mill %&gt;% mutate(after.1980 = if_else(year &gt;= 1980, 1, 0)) ## Creating a vector of the years of interest specific.years &lt;- c(1950, 1980, 2000) ## Adding a variable with if_else and %in% UNpop.mill &lt;- UNpop.mill %&gt;% mutate(year.of.interest = if_else(year %in% specific.years, 1, 0)) summary(UNpop.mill) ## year world.pop.mill after.1980 year.of.interest ## Min. :1950 Min. :2526 Min. :0.0000 Min. :0.0000 ## 1st Qu.:1965 1st Qu.:3359 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1980 Median :4449 Median :1.0000 Median :0.0000 ## Mean :1980 Mean :4580 Mean :0.5714 Mean :0.4286 ## 3rd Qu.:1995 3rd Qu.:5724 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :2010 Max. :6916 Max. :1.0000 Max. :1.0000 mean(UNpop.mill$world.pop.mill) ## [1] 4579.529 ## Add a row where values for all columns is NA UNpop.mill.wNAs &lt;- UNpop.mill %&gt;% add_row(year = NA, world.pop.mill = NA, after.1980 = NA, year.of.interest = NA) ## Take the mean of world.pop.mill (returns NA) mean(UNpop.mill.wNAs$world.pop.mill) ## [1] NA ## Take the mean of world.pop.mill (ignores the NA) mean(UNpop.mill.wNAs$world.pop.mill, na.rm = TRUE) ## [1] 4579.529 ## Other Summary Statistics with tidyverse UNpop.mill %&gt;% summarize(mean.pop = mean(world.pop.mill), median.pop = median(world.pop.mill)) ## mean.pop median.pop ## 1 4579.529 4449.049 UNpop.mill %&gt;% group_by(after.1980) %&gt;% # create subset group for each value of after.1980 summarize(mean.pop = mean(world.pop.mill)) # calculate mean for each group ## # A tibble: 2 × 2 ## after.1980 mean.pop ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3081. ## 2 1 5703. "],["lab-ii-introduction-to-librarytidyverse-r-markdown.html", "3 Lab II: Introduction to library(tidyverse) &amp; R Markdown 3.1 In the setup chunk above, load the tidyverse packages as well as library(readr) 3.2 Load in the resume.RData file and use head(), tail(), glimpse(), dim(), summary(), and View() to examine each variable in the dataset. How many of the resumes have white sounding names? How many have African-American sounding names. 3.3 This experiment seeks to determine whether or not hiring managers discriminate on the basis of racial identity by sending idential resumes with African-American and white sounding names to job postings. The basic logic is that resumes are identical and only the name is changing, so any differences in call backs for jobs can be attributed to racial discrimination. Why do the authors want to randomize? And do you think this is an effective research design? 3.4 We are going to see if there is a racial discrepency by taking the difference in callback rates between racial groups. Calculate the callback rate for white sounding name applicants and African-American sounding name applicants. Use Latex commands to write the formula for this calculation and display the result in text. Write the formula between $’s like \\(y = mx + b\\) to use Latex commands. 3.5 Now, create a new object that stores the difference in callback rates named race_diff. 3.6 Since Crenshaw (1989), manny scholars have concerned with intersectionality, or how race and gender interact to make the experiences of African-American women unique. We can use the data we have to explore the effect of race and gender specific sounding names on employment prospects. Calculate the call back rate by each race and gender category. 3.7 What is the difference in call back rates for each race/gender group?", " 3 Lab II: Introduction to library(tidyverse) &amp; R Markdown We can use R Markdown to create well-formatted PDFs or .html files that can easily display the results of our analyses. R Markdown, through Latex, also allows to write mathematical formulas with ease. Go ahead and knit - found in the top left corner - this file now and see what it looks like. 3.1 In the setup chunk above, load the tidyverse packages as well as library(readr) ## Example Setup Chunk knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) ## Packages library(readr) library(tidyverse) 3.2 Load in the resume.RData file and use head(), tail(), glimpse(), dim(), summary(), and View() to examine each variable in the dataset. How many of the resumes have white sounding names? How many have African-American sounding names. ## Loading Data data(resume, package = &quot;qss&quot;) ## Learning About the Dataset head(resume) ## firstname sex race call ## 1 Allison female white 0 ## 2 Kristen female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 ## 5 Carrie female white 0 ## 6 Jay male white 0 tail(resume) ## firstname sex race call ## 4865 Lakisha female black 0 ## 4866 Tamika female black 0 ## 4867 Ebony female black 0 ## 4868 Jay male white 0 ## 4869 Latonya female black 0 ## 4870 Laurie female white 0 glimpse(resume) ## Rows: 4,870 ## Columns: 4 ## $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;, &quot;Jay&quot;, &quot;Jill&quot;, &quot;Kenya&quot;, &quot;Latonya&quot;, &quot;Tyrone&quot;, … ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fe… ## $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, … ## $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … dim(resume) ## [1] 4870 4 summary(resume) ## firstname sex race call ## Length:4870 Length:4870 Length:4870 Min. :0.00000 ## Class :character Class :character Class :character 1st Qu.:0.00000 ## Mode :character Mode :character Mode :character Median :0.00000 ## Mean :0.08049 ## 3rd Qu.:0.00000 ## Max. :1.00000 View(resume) ## Comment this out when knitting. ## Number of observations by race resume %&gt;% group_by(race) %&gt;% count() ## # A tibble: 2 × 2 ## # Groups: race [2] ## race n ## &lt;chr&gt; &lt;int&gt; ## 1 black 2435 ## 2 white 2435 3.3 This experiment seeks to determine whether or not hiring managers discriminate on the basis of racial identity by sending idential resumes with African-American and white sounding names to job postings. The basic logic is that resumes are identical and only the name is changing, so any differences in call backs for jobs can be attributed to racial discrimination. Why do the authors want to randomize? And do you think this is an effective research design? The concerns of examining race and the number of callbacks without randomization is that there could be confounders like workplace connections, amount of education, and employment history that could be correlated with race. It is possible that African-American applicants did not have the same employment and educational opportunities as white Americans, and, therefore, their resumes may look significantly different. This raises issues of counfounding and makes it impossible to differentiate if an employer made their decision based on race or based on the substance of the resume. The authors, though, randomized race, reducing this risk of confounding. Employers in this study are seeing nearly identical resumes, with only the race of the applicant being different, as indicated by a name. The field experiment presented here relies on racial connotations of different names, not explicit racial cues. Therefore, hiring managers are determining an applicant’s race largely based on what scholars of identity politics would call “perceived race” or “street race” (Lopez et al. 2017), which is how others perceive an individual’s race. This fact means that the selection of names is integral to the internal validity of the research design. 3.4 We are going to see if there is a racial discrepency by taking the difference in callback rates between racial groups. Calculate the callback rate for white sounding name applicants and African-American sounding name applicants. Use Latex commands to write the formula for this calculation and display the result in text. Write the formula between $’s like \\(y = mx + b\\) to use Latex commands. ## Call Back for white Sounding Name Applicants resume %&gt;% group_by(race) %&gt;% summarise(callback_rates = mean(call)) ## # A tibble: 2 × 2 ## race callback_rates ## &lt;chr&gt; &lt;dbl&gt; ## 1 black 0.0645 ## 2 white 0.0965 The callback rate for whites is .096. We take the mean of the binary callback variable, \\(\\overline{x} = \\frac{1}{n}\\Sigma^{n}_{i=1}x_i\\) The callback rate for African-American sounding name applicants is .064. 3.5 Now, create a new object that stores the difference in callback rates named race_diff. ## Calculating Callback Proportions race_call &lt;- resume %&gt;% group_by(race, call) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## Difference in call back rates race_diff &lt;- race_call %&gt;% select(race, call_prop) %&gt;% pivot_wider(names_from = c(race), values_from = call_prop) %&gt;% mutate(race_diff = white - black) %&gt;% select(race_diff) ## Printing race_diff ## # A tibble: 1 × 1 ## race_diff ## &lt;dbl&gt; ## 1 0.0320 3.6 Since Crenshaw (1989), manny scholars have concerned with intersectionality, or how race and gender interact to make the experiences of African-American women unique. We can use the data we have to explore the effect of race and gender specific sounding names on employment prospects. Calculate the call back rate by each race and gender category. ## Callbacks by race and gender resume %&gt;% group_by(race, call, sex) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## # A tibble: 4 × 6 ## # Groups: race, sex [4] ## race sex no_call call total_resumes call_prop ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 black female 1761 125 1886 0.0663 ## 2 black male 517 32 549 0.0583 ## 3 white female 1676 184 1860 0.0989 ## 4 white male 524 51 575 0.0887 3.7 What is the difference in call back rates for each race/gender group? ## Saving tibble from 8 dta &lt;- resume %&gt;% group_by(race, call, sex) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## Calculating Differences call_backs &lt;- dta %&gt;% select(race, sex, call_prop) %&gt;% pivot_wider(names_from = c(sex, race), values_from = call_prop) %&gt;% mutate(white_sex_diff = female_white - male_white, black_sex_diff = female_black - male_black, male_race_diff = male_white - male_black, female_race_diff = female_white - female_black) %&gt;% select(white_sex_diff, black_sex_diff, male_race_diff, female_race_diff) ## Printing print(call_backs) ## print() is optional ## # A tibble: 1 × 4 ## white_sex_diff black_sex_diff male_race_diff female_race_diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0102 0.00799 0.0304 0.0326 "],["lab-iii-univariate-visualizations.html", "4 Lab III: Univariate Visualizations 4.1 Histograms with library(ggplot2) 4.2 Barplots with library(ggplot2) 4.3 Now Make Your Own Histogram or Barplot!", " 4 Lab III: Univariate Visualizations ## Packages library(tidyverse) library(ggthemes) ## Data Loading ## Replace this with your working directory load(&quot;~/GOVT8001/Lab 3/white_minwage.RData&quot;) This lab shows step-by-step how to build basic histograms and barplots with library(ggplot2) 4.1 Histograms with library(ggplot2) Histograms are good to visualize the distribution of one continuous variable. 4.1.1 Step One Specify the tibble to be piped into ggplot() ## Building A Basic Histogram df.county 4.1.2 Step Two Pipe the tibble into ggplot() Specify the variable of interest with ggplot(aes(x = X)) ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) 4.1.3 Step 3 Use + instead of %&gt;% to move to next line in ggplot() geom_histogram() creates the histogram ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..)) 4.1.4 Step 4 Customization of theme, colors, and labels. You can also save the object above and customize it later as shown below Use col = and fill = in geom_histogram() to set colors ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) Use + theme() to set the theme library(ggtheme) has themes from your favorite publications! ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) + theme_minimal() Use + labs to set labels title = for a title subtitle = for a subtitle x = for x axis label and y = for y axis label caption = for caption to include data source or note ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) + theme_minimal() + labs(title = &quot;Distribution of Minimum Wage&quot;, subtitle = &quot;All US Counties 1996 - 2016&quot;, x = &quot;Minimum Wage&quot;, caption = &quot;Data Source: Markovich &amp; White (2022)&quot;, y = &quot;Density&quot;) ## Or You Can Save the Basic Plot and Experiment p &lt;- df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) p p + theme_minimal() + labs(title = &quot;Distribution of Minimum Wage&quot;, subtitle = &quot;All US Counties 1996 - 2016&quot;, x = &quot;Minimum Wage&quot;, caption = &quot;Data Source: Markovich &amp; White (2022)&quot;, y = &quot;Density&quot;) 4.2 Barplots with library(ggplot2) Barplots are good for visualizing distributions by groups. The steps here follow closely what we did for the histogram. 4.2.1 Step One We will be using simulated data for this example. First we need to format our simulated data into something we can use for the barplot with the skills we learned last week. ## Simulated Data df &lt;- data.frame(&quot;age&quot; = c(&quot;18 to 29&quot;, &quot;36 to 50&quot;, &quot;51 to 64&quot;, &quot;65+&quot;), &quot;popPct&quot; = c(29, 21, 30, 20), &quot;svyPct&quot; = c(19, 21, 32, 28)) df ## age popPct svyPct ## 1 18 to 29 29 19 ## 2 36 to 50 21 21 ## 3 51 to 64 30 32 ## 4 65+ 20 28 ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) ## # A tibble: 8 × 3 ## age Group Percent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 18 to 29 Population 29 ## 2 18 to 29 Survey 19 ## 3 36 to 50 Population 21 ## 4 36 to 50 Survey 21 ## 5 51 to 64 Population 30 ## 6 51 to 64 Survey 32 ## 7 65+ Population 20 ## 8 65+ Survey 28 4.2.2 Step Two Pipe the tibble into ggplot() Specify the variable of interest with ggplot(aes(x = X)) Since we want to show the distribution of X by some group, we can use fill = to specify the group ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) 4.2.3 Step 3 Use + instead of %&gt;% to move to next line in ggplot() geom_bar() creates a barplot ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) 4.2.4 Step 4 Now, we can customize just like above with the histogram. scale_fill_grey() changes the color palette to greyscale ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_grey() + theme_minimal() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 4.3 Now Make Your Own Histogram or Barplot! df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;pink&quot;, fill = &quot;black&quot;) + theme_economist() + labs(title = &quot;Our Beautiful Plot&quot;) "],["lab-iv-loops-lists-conditional-statements.html", "5 Lab IV: Loops, Lists, &amp; Conditional Statements 5.1 Lists 5.2 Indexing, Conditional Statements, &amp; if_else() 5.3 Loops 5.4 Lab Questions 5.5 Practice Creating Your Own Loops", " 5 Lab IV: Loops, Lists, &amp; Conditional Statements ## Packages library(tidyverse) library(mosaic) ## for the Saratoga Houses Dataset ## Data data(&quot;SaratogaHouses&quot;) 5.1 Lists 5.1.1 What is a list? A list is what is called a recursive vector A recursive vector is a vector than can contain other vectors or lists Think of lists intuitively as a more flexible vector that can contain individual vectors and even dataframes/tibbles ## Creating A List presidents &lt;- c(&quot;Washington&quot;, &quot;Adams&quot;, &quot;Jefferson&quot;, &quot;Madison&quot;, &quot;Monroe&quot;) chief_justices &lt;- c(&quot;Marshall&quot;, &quot;Warren&quot;, &quot;Burger&quot;, &quot;Rehnquist&quot;, &quot;Roberts&quot;) ages &lt;- c(51, 82, 12, 18, 43) df &lt;- data.frame(presidents, chief_justices) list_1 &lt;- list(presidents, chief_justices, ages, SaratogaHouses) ## Accessing specific objects in the list list_1[[3]] ## [1] 51 82 12 18 43 list_1[[1]] ## [1] &quot;Washington&quot; &quot;Adams&quot; &quot;Jefferson&quot; &quot;Madison&quot; &quot;Monroe&quot; 5.2 Indexing, Conditional Statements, &amp; if_else() We can pull specific values of a variable out with [] This works with vectors and dataframes/tibbles ## Pulling 4th observation of ages ages[4] ## [1] 18 ## Pulling 1st observation of presidents presidents[1] ## [1] &quot;Washington&quot; ## Pulling 2-4th observation of presidents presidents[2:4] ## [1] &quot;Adams&quot; &quot;Jefferson&quot; &quot;Madison&quot; ## Pulling all except the 3rd-5th observation of ages ages[-3] ## [1] 51 82 18 43 ## Example with a dataframe/tibble SaratogaHouses[2,4] ## [1] 22300 SaratogaHouses[1:3, 4:7] ## landValue livingArea pctCollege bedrooms ## 1 50000 906 35 2 ## 2 22300 1953 51 3 ## 3 7300 1944 51 4 You can also use indexing to pull out specific observations of a variable ## Finding the prices for all houses with 3 bathrooms SaratogaHouses$price[SaratogaHouses$bathrooms == 3] ## [1] 248800 169900 293000 205000 240000 235000 187000 293565 285558 133000 250000 322277 336000 359770 303374 500075 ## [17] 425000 465000 285000 374900 405000 278500 229000 420000 225000 349900 349900 330000 244900 253000 375000 262000 ## [33] 725000 455000 430000 397200 275055 272699 287989 300279 304065 314940 440760 255000 284595 342269 297065 345264 ## [49] 319000 307890 410000 395000 360000 281520 225570 314230 ## With dplyr SaratogaHouses %&gt;% filter(bathrooms == 3) %&gt;% pull(price) ## [1] 248800 169900 293000 205000 240000 235000 187000 293565 285558 133000 250000 322277 336000 359770 303374 500075 ## [17] 425000 465000 285000 374900 405000 278500 229000 420000 225000 349900 349900 330000 244900 253000 375000 262000 ## [33] 725000 455000 430000 397200 275055 272699 287989 300279 304065 314940 440760 255000 284595 342269 297065 345264 ## [49] 319000 307890 410000 395000 360000 281520 225570 314230 These conditional statements can get more complicated as well ## Finding the average price of a house with 5 bedrooms, 2 bathrooms, and a fireplace mean(SaratogaHouses$price[SaratogaHouses$bedrooms == 5 &amp; SaratogaHouses$bathrooms == 3 &amp; SaratogaHouses$fireplaces &gt; 0]) ## [1] 256506.7 ## With dplyr SaratogaHouses %&gt;% filter(bedrooms == 5, bathrooms == 3, fireplaces &gt; 0) %&gt;% summarise(avg_price = mean(price)) ## avg_price ## 1 256506.7 ## Finding the cheapest house with 3 bedrooms on the water min(SaratogaHouses$price[SaratogaHouses$bedrooms == 3 &amp; SaratogaHouses$waterfront == &quot;Yes&quot;]) ## [1] 319000 ## With dply SaratogaHouses %&gt;% filter(bedrooms == 3, waterfront == &quot;Yes&quot;) %&gt;% summarise(cheapest_house = min(price)) ## cheapest_house ## 1 319000 You can also use if_else() to create new variables in mutate() ## Showing if_else() SaratogaHouses %&gt;% mutate(fireplace = if_else(fireplaces &gt; 0, 1, 0)) %&gt;% select(fireplace) %&gt;% slice(1:5) ## fireplace ## 1 1 ## 2 0 ## 3 1 ## 4 1 ## 5 0 SaratogaHouses %&gt;% mutate(large_house = if_else(rooms &gt; mean(rooms), 1, 0)) %&gt;% select(large_house) %&gt;% slice(1:5) ## large_house ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 5.3 Loops 5.3.1 What is a Loop? A central concept of programming that is found in most programming languages Loops are control statements that execute one or more statements for a desired number of times Loops can be used to iterate applying a function a certain number of times to a specified object(s) 5.3.2 How Loops Work in R ## Basic Loop for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Without print() What happened? ## Basic Loop for (i in 1:5) { i } Works for character and numeric vectors too ## Character Vector parties &lt;- c(&quot;Democratic&quot;, &quot;Republican&quot;, &quot;Libertarian&quot;, &quot;Green&quot;) for (i in parties) { print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Republican&quot; ## [1] &quot;Libertarian&quot; ## [1] &quot;Green&quot; ## Numeric Vector numbers &lt;- c(1, 2, 3, 4, 5) for (i in numbers) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Let’s write a loop that applies the square root function to a vector of integers ## Square Root Loop for (i in 1:length(numbers)) { print(sqrt(i)) } ## [1] 1 ## [1] 1.414214 ## [1] 1.732051 ## [1] 2 ## [1] 2.236068 5.3.3 Conditional Statements &amp; Stopping Loops ## A Loop That Stops for (i in parties) { if (i == &quot;Libertarian&quot;) { break } print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Republican&quot; 5.3.4 Conditional Statements &amp; Skipping Iterations ## A Loop That Skips An Iteration for (i in parties) { if (i == &quot;Republican&quot;) { next } print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Libertarian&quot; ## [1] &quot;Green&quot; 5.3.5 if else Statements ## Using if else for (i in numbers) { if (i &gt; 3) { print(&quot;Number Greater Than 3&quot;) } else { print(&quot;Number Less Than 4&quot;) } print(i) } ## [1] &quot;Number Less Than 4&quot; ## [1] 1 ## [1] &quot;Number Less Than 4&quot; ## [1] 2 ## [1] &quot;Number Less Than 4&quot; ## [1] 3 ## [1] &quot;Number Greater Than 3&quot; ## [1] 4 ## [1] &quot;Number Greater Than 3&quot; ## [1] 5 5.3.6 More Complicated Loops What is going on here? page 146 in Imai &amp; Williams (2022) ## Example from QSS values &lt;- c(2, 4, 6) n &lt;- length(values) results &lt;- rep(NA, n) for (i in seq_along(values)) { results[i] &lt;- values[i] * 2 print(str_c(values[i], &quot; times 2 is equal to &quot;, results[i])) } ## [1] &quot;2 times 2 is equal to 4&quot; ## [1] &quot;4 times 2 is equal to 8&quot; ## [1] &quot;6 times 2 is equal to 12&quot; What is going on here? ## Loop to Calculate A Series of Means for (i in 1:length(unique(SaratogaHouses$bedrooms))) { x &lt;- mean(SaratogaHouses$price[SaratogaHouses$bedrooms == i]) names(x) &lt;- i print(x) } ## 1 ## 192771.4 ## 2 ## 152561.3 ## 3 ## 200678 ## 4 ## 265550.6 ## 5 ## 276577.5 ## 6 ## 277328.8 ## 7 ## 226666.7 Same thing with library(tidyverse) ## Using dplyr() SaratogaHouses %&gt;% group_by(bedrooms) %&gt;% summarise(avg_price = mean(price)) ## # A tibble: 7 × 2 ## bedrooms avg_price ## &lt;int&gt; &lt;dbl&gt; ## 1 1 192771. ## 2 2 152561. ## 3 3 200678. ## 4 4 265551. ## 5 5 276578. ## 6 6 277329. ## 7 7 226667. 5.3.7 Conditional Means A conditional mean is simply the mean of some variable given when a certain set of conditions are met. We do this in R by indexing and subsetting. As an example, assume that you may be interested in voter turnout by identity group. Thus, you are calculating the mean of voter turnout conditional on identity status. Remember this for regression to help intuitively understand what OLS is doing. 5.3.8 Factor Variables A factor variable is a categorical variable that can only take a distinct set of values. An example is marital status which could take single, married, or divorced. A categorical variable like a people’s names is not a factor variable as it could essentially take an infinite number of possible values. 5.4 Lab Questions 5.4.1 Use a loop and library(dplyr) to calculate the maximum price of a house conditional on the number of rooms that a house has. ## Loop for (i in 1:length(unique(SaratogaHouses$rooms))) { x &lt;- mean(SaratogaHouses$price[SaratogaHouses$rooms == i], na.rm = T) names(x) &lt;- i print(x) } ## 1 ## NaN ## 2 ## 94500 ## 3 ## 134156.2 ## 4 ## 168917.7 ## 5 ## 167297.6 ## 6 ## 185313.7 ## 7 ## 191829 ## 8 ## 220596.8 ## 9 ## 245278.8 ## 10 ## 288567.2 ## 11 ## 305913.7 ## With dplyr SaratogaHouses %&gt;% group_by(rooms) %&gt;% summarise(avg_price = mean(price)) ## # A tibble: 11 × 2 ## rooms avg_price ## &lt;int&gt; &lt;dbl&gt; ## 1 2 94500 ## 2 3 134156. ## 3 4 168918. ## 4 5 167298. ## 5 6 185314. ## 6 7 191829. ## 7 8 220597. ## 8 9 245279. ## 9 10 288567. ## 10 11 305914. ## 11 12 373219. 5.4.2 Create a loop that calculates the maximum and minimum house prices by number of bedrooms. Replicate with library(dplyr) ## With Loop for (i in 1:length(unique(SaratogaHouses$bedrooms))) { max_price &lt;- max(SaratogaHouses$price[SaratogaHouses$bedrooms == i], na.rm = T) min_price &lt;- min(SaratogaHouses$price[SaratogaHouses$bedrooms == i], na.rm = T) names(max_price) &lt;- i names(min_price) &lt;- i print(c(max_price, min_price)) } ## 1 1 ## 315000 78000 ## 2 2 ## 655000 10300 ## 3 3 ## 775000 5000 ## 4 4 ## 725000 65000 ## 5 5 ## 775000 119900 ## 6 6 ## 422680 95000 ## 7 7 ## 325000 131000 ## With dplyr SaratogaHouses %&gt;% group_by(bedrooms) %&gt;% summarise(max_price = max(price), min_price = min(price)) ## # A tibble: 7 × 3 ## bedrooms max_price min_price ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 315000 78000 ## 2 2 655000 10300 ## 3 3 775000 5000 ## 4 4 725000 65000 ## 5 5 775000 119900 ## 6 6 422680 95000 ## 7 7 325000 131000 5.4.3 What is the most number of fireplaces in a house with five bedrooms? ## Most Fireplace in 5 bedroom house max(SaratogaHouses$fireplaces[SaratogaHouses$bedrooms == 5]) ## [1] 4 5.5 Practice Creating Your Own Loops Use SaratogaHouses or use the data() command to bring in a dataset of your interest and write a loop to carry out an easy task. "],["lab-v-introduction-to-regression.html", "6 Lab V: Introduction to Regression 6.1 Correlation 6.2 Introduction to Regression 6.3 Lab Questions", " 6 Lab V: Introduction to Regression 6.1 Correlation Statistical technique that tells use the extent that two variables are linearly related \\[r=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\sum_{n}^{i=1}(x_{i}-\\bar{x})^2\\sum_{n}^{i=1}(y_{i}-\\bar{y})^2}}\\] \\[r = \\frac{Cov(X,Y)}{SD(X)SD(Y)}\\] Bounded between \\(-1,1\\) In R, cor(x,y) Scatterplot in R geom_point() 6.1.1 Correlation &amp; Scatterplots 6.2 Introduction to Regression Why Regression? Basic Model Conditional Mean Estimation Coefficients &amp; Intercepts 6.2.1 Why Regression? Regression is the work-horse of quantitative social science We use regression when we want to know the relationship between \\(x\\) and \\(y\\) Specifically, we want to know what a one unit increase in \\(x\\) means for \\(y\\) Real Data vs Hats Hats are expected values \\(y\\) vs \\(\\hat{y}\\) Identify a variable of interest, we call this \\(y\\) Ex: vote share, approval rating, gdp, etc. Take its mean, \\(\\bar{y}\\) We have variation in our \\(y\\) variable, though, e.g., some values are high and others are low Next find a variable that we think explains this variation, we call this \\(x\\) Then estimate the relationship between \\(x\\) and \\(y\\) We call \\(x\\) the treatment variable, causal variable, independent variable, predictor variable, explanatory variable We call \\(y\\) the dependent variable, outcome variable, response variable 6.2.2 Correlation vs Regression Similarities Both show the direction and strength of the relationship between two variables When correlation is negative, \\(r&lt;0\\), then the regression slope will be negative, \\(\\beta_{1}&lt;0\\) When correlation is positive, the regression slope will be positive Differences Regression is for causation (with additional steps) while correlation is for, well, correlation \\(X\\) and \\(Y\\) are interchangeable in correlation, but results will change if you swap \\(X\\) and \\(Y\\) in regression Correlation is a single statistic while regression gives us an equation 6.2.3 Basic Model \\[\\hat{y_{i}} = \\hat{\\beta_{0}} + \\hat{\\beta_{1}}x_{i}\\] analogous to \\[y = mx + b\\] Where: \\(\\hat{y_{i}} = y\\), \\(\\hat{\\beta_{0}} = b\\), \\(\\hat{\\beta_{1}}\\) = m, and \\(x_{i}=x\\) \\(\\beta_{1}\\) is the slope \\(\\beta_{0}\\) is the y-intercept Example of a model you may (will) see in the comparative politics literature: \\[GDPGrowth_{i} = \\beta_{0} + \\beta_{1}Democracy_{i} + \\epsilon_{i}\\] Why no hats? 6.2.4 Conditional Mean Regression tells us the conditional mean of \\(y\\) given \\(x\\) \\(E[Y|X] = \\beta_{0} + \\beta_{1}X\\) ## Mean of All Children&#39;s Heights mean(Galton$height) ## [1] 66.76069 ## Mean of Female Children&#39;s Heights mean(Galton$height[Galton$sex == &quot;F&quot;]) ## [1] 64.11016 ## Mean of Male Children&#39;s Heights mean(Galton$height[Galton$sex == &quot;M&quot;]) ## [1] 69.22882 ## Conditional Mean with Regression lm(height ~ sex, data = Galton) ## ## Call: ## lm(formula = height ~ sex, data = Galton) ## ## Coefficients: ## (Intercept) sexM ## 64.110 5.119 6.2.5 Estimation \\[\\hat{\\beta}=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sum_{n}^{i=1}(x_{i}-\\bar{x})^2}\\] \\[\\hat{\\beta}= \\frac{Cov(x,y)}{Var(x)}\\] Intuitively, \\(\\hat{\\beta}\\) is the variance of \\(x\\) and \\(y\\) together divided by the variance of \\(x\\) Thus, you are left only with the variation in \\(y\\) caused by \\(x\\) The main result of estimation is the coefficient and intercept In R, lm(y ~ x) 6.2.6 Coefficients &amp; Intercepts Coefficient A one unit increase in \\(x\\) is associated with a \\(\\hat{\\beta_{1}}\\) increase in \\(y\\) Be very careful about the units of \\(x\\) For example, an \\(x\\) that takes the values 0-1, a one unit increase is the full range of \\(x\\) Intercept The fitted value of \\(y\\) when \\(x=0\\) We multiply any given value of x by the constant \\(\\hat{\\beta_{1}}\\) and add the intercept to get \\(\\hat{y}\\), the fitted value of y 6.2.7 Scatterplots &amp; Regression Lines - How do we interpret \\(\\hat{\\beta}\\) and \\(\\hat{\\alpha}\\)? - What do you expect a child’s height will be if their father’s height is 70 inches? ## Mean when father is 70 inches tall mean(Galton$height[Galton$father == 70]) ## [1] 66.94885 How would we find the residuals? \\(\\hat{\\epsilon} = y_i - \\hat{y_i}\\) 6.3 Lab Questions Download the Correlates of War dataset from Canvas. We are going to learn how to make scatterplots, find correlations, and run simple regressions. Sarkees, Meredith Reid and Frank Wayman (2010). Resort to War: 1816 – 2007. Washington DC: CQ Press. 6.3.1 Question I - Load in the Data. ## Loading in Data library(readr) cow &lt;- read_csv(&quot;~/GOVT8001/Lab 5/Intra-StateWarData_v4.1.csv&quot;) 6.3.2 Question II - Create a scatterplot to show the joint distribution of war deaths by each side. Experiment with subsetting the data to different values of war deaths and visualization styles. Also, be careful about missing data and unknown values. Add a regression line. ## Scatterplot cow %&gt;% filter(SideADeaths &lt; 20000 &amp; SideADeaths &gt; 0, SideBDeaths &lt; 20000 &amp; SideBDeaths &gt; 0, EndYear1 &gt;= 0) %&gt;% ggplot(aes(x = SideADeaths, y = SideBDeaths)) + geom_point() + geom_abline(method = &quot;lm&quot;, col = &quot;dark blue&quot;) + theme_bw() + labs(title = &quot;Joint Distribution of War Deaths&quot;) 6.3.3 Question III - What is the correlation coefficient for the relationship you depicted in Question II. What is the regression coefficient? How do we interpret each? Only filter out the less than zero observations for this question. ## Filtering Data cow_filtered &lt;- cow %&gt;% filter(SideADeaths &gt; 0, SideBDeaths &gt; 0) ## Correlation cor(cow_filtered$SideADeaths, cow_filtered$SideBDeaths) ## [1] 0.8043606 ## Regression cow %&gt;% filter(SideADeaths &gt; 0, SideBDeaths &gt; 0) %&gt;% lm(SideBDeaths ~ SideADeaths, data = .) %&gt;% summary() ## ## Call: ## lm(formula = SideBDeaths ~ SideADeaths, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -191383 -3771 -2030 -10 515351 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2924.15036 3652.12170 0.801 0.424 ## SideADeaths 1.08097 0.06124 17.652 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 46330 on 170 degrees of freedom ## Multiple R-squared: 0.647, Adjusted R-squared: 0.6449 ## F-statistic: 311.6 on 1 and 170 DF, p-value: &lt; 0.00000000000000022 6.3.4 Question IV - Use both regression - the lm() function and the mean() function to show the difference between total war deaths depending on if a war was internationalized or not. Create a total deaths variable for this question. ## Difference in Means Based on Internationalization with Regression cow_filtered %&gt;% mutate(total_deaths = SideADeaths + SideBDeaths) %&gt;% lm(total_deaths ~ Intnl, data = .) %&gt;% summary() ## ## Call: ## lm(formula = total_deaths ~ Intnl, data = .) ## ## Residuals: ## Min 1Q Median 3Q Max ## -60572 -25549 -23574 -16024 1173351 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26649 11102 2.400 0.0175 * ## Intnl 34922 23621 1.478 0.1411 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 128500 on 170 degrees of freedom ## Multiple R-squared: 0.01269, Adjusted R-squared: 0.006887 ## F-statistic: 2.186 on 1 and 170 DF, p-value: 0.1411 ## With Mean cow_filtered %&gt;% mutate(total_deaths = SideADeaths + SideBDeaths) %&gt;% group_by(Intnl) %&gt;% summarize(avg_deaths = mean(total_deaths)) ## # A tibble: 2 × 2 ## Intnl avg_deaths ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 26649. ## 2 1 61572. ## Differences ## With Mean cow_filtered %&gt;% mutate(total_deaths = SideADeaths + SideBDeaths) %&gt;% filter(Intnl == 1) %&gt;% summarize(non_int_deaths = mean(total_deaths)) %&gt;% pull(non_int_deaths) - cow_filtered %&gt;% mutate(total_deaths = SideADeaths + SideBDeaths) %&gt;% filter(Intnl == 0) %&gt;% summarize(non_int_deaths = mean(total_deaths)) %&gt;% pull(non_int_deaths) ## [1] 34922.34 6.3.5 Question V - Write a loop to find the average total deaths based on the month the war started in. Do the same with dplyr. ## Wrangling Data cow_filtered &lt;- cow_filtered %&gt;% filter(StartMonth1 &gt; 0) %&gt;% mutate(total_deaths = SideADeaths + SideBDeaths) ## Loop for (i in unique(cow_filtered$StartMonth1)) { x &lt;- mean(cow_filtered$total_deaths[cow_filtered$StartMonth1 == i]) names(x) &lt;- i print(x) } ## 6 ## 12310.33 ## 3 ## 62869.5 ## 7 ## 64465.71 ## 2 ## 7942.857 ## 8 ## 8314.538 ## 10 ## 21915.77 ## 4 ## 62369.33 ## 11 ## 48016.43 ## 5 ## 6795 ## 9 ## 14526.15 ## 1 ## 15822.15 ## 12 ## 75963.08 ## With dplyr cow_filtered %&gt;% group_by(StartMonth1) %&gt;% summarise(avg_death_by_month = mean(total_deaths)) ## # A tibble: 12 × 2 ## StartMonth1 avg_death_by_month ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 15822. ## 2 2 7943. ## 3 3 62870. ## 4 4 62369. ## 5 5 6795 ## 6 6 12310. ## 7 7 64466. ## 8 8 8315. ## 9 9 14526. ## 10 10 21916. ## 11 11 48016. ## 12 12 75963. "],["lab-vi-goodness-of-fit-multiple-regression.html", "7 Lab VI: Goodness of Fit &amp; Multiple Regression 7.1 \\(R^2\\) 7.2 Mean Squared Error &amp; Root Mean Squared Error 7.3 Multiple Regression 7.4 Adjusted \\(R^2\\) 7.5 Lab Questions: Congratulations! You’ve been hired as an analyst by the [insert your favorite baseball team name here]. Your first job is to make a few predictions about what your teams’ future will look like.", " 7 Lab VI: Goodness of Fit &amp; Multiple Regression ## Packages library(tidyverse) library(tidymodels) library(mosaic) ## Data load(&quot;~/GOVT5001/Lab VI/sh.RData&quot;) house &lt;- SaratogaHouses ## Model model1 &lt;- lm(price~bedrooms, data=house) 7.1 \\(R^2\\) \\[1-\\frac{SSR}{TSS} = \\frac{\\sum\\epsilon^2}{\\sum(y_{i}-\\bar{y})^2}=\\frac{MSS}{TSS}=\\frac{\\sum(\\hat{y_{i}}-\\bar{y})^2}{\\sum(y_{i}-\\bar{y})^2}\\] Coefficient of Determination Proportion of the Variance in \\(Y_{i}\\) explained by \\(X_{i}\\) “Goodness of Fit” ##1-SSR/TSS 1-sum((model1$residuals)^2)/ sum((house$price-mean(house$price))^2) ## [1] 0.1602791 ## MSS/TSS sum((model1$fitted.values-mean(house$price))^2)/ sum((house$price-mean(house$price))^2) ## [1] 0.1602791 summary(model1)$r.squared ## [1] 0.1602791 \\[R^2=Cor(Y_i,\\hat{Y_i})\\] Squared Correlation Between Observed Y and Fitted Y mod_aug &lt;- augment(model1) (cor(mod_aug$.fitted, mod_aug$price)^2) ## [1] 0.1602791 \\(R^2\\) as proportional reduction in error ## Constant Only model null_model &lt;- lm(house$price~1) ## The intercept is just the mean of Y summary(null_model)$coefficient ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 211966.7 2368.132 89.50798 0 mean(house$price) ## [1] 211966.7 ## R Squared is 0 summary(null_model)$r.squared ## [1] 0 ## Squared of Model 1 summary(model1)$r.squared ## [1] 0.1602791 The number of bedrooms that a house has explains 16% of the variation in the price of a house 7.2 Mean Squared Error &amp; Root Mean Squared Error \\[\\hat{MSE} = \\frac{1}{n}\\sum^{n}_{i=1}(Y_{i}-\\hat{Y_{i}})^2\\] Average distance from the line-of-best-fit The root mean square error, or RMSE, is calculated by taking the square root of the MSE. Here is the formula: \\[\\hat{RMSE} = \\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}(Y_{i}-\\hat{Y_{i}})^2}\\] The average prediction error in units of Y fit &lt;- lm(Mustangs$Price ~ Mustangs$Miles) Mustangs$predicted &lt;- predict(fit) Mustangs$residuals &lt;- residuals(fit) Mustangs %&gt;% ggplot(aes(Miles, Price)) + geom_point(col = &quot;black&quot;, size = 3) + geom_point(aes(y = predicted), shape = 1, size = 3) + geom_segment(aes(xend = Miles, yend = predicted)) + theme_bw() + geom_smooth(method = &quot;lm&quot;, se = F) + labs(x = &quot;X&quot;, y = &quot;Y&quot;) Take the square root of the mean of the distance of the solid black lines for RMSE ## MAE of Model 1 summary(model1)$sigma ## [1] 90234.16 ## Manually sqrt(mean((mod_aug$.fitted - mod_aug$price)^2)) ## [1] 90181.93 7.3 Multiple Regression \\[Price_{i}=\\beta_{0} + \\beta_{1}Bedrooms_{i} + \\beta_{2}Bathrooms_{i} + \\epsilon_{i}\\] lm(price~bedrooms + bathrooms, data=house) ## ## Call: ## lm(formula = price ~ bedrooms + bathrooms, data = house) ## ## Coefficients: ## (Intercept) bedrooms bathrooms ## 2193 19325 78316 7.4 Adjusted \\(R^2\\) \\[Adjusted\\space R^2 = 1- \\frac{(1-R^2)(N-1)}{(N-k-1)}\\] \\(R^2\\) is just \\(R^2\\) \\(N\\) is the sample size \\(k\\) is the number of independent variables Penalizes us for just throwing more variables at the model Proportion of variation in \\(Y_{i}\\) explained by the model model2 &lt;- lm(price~bedrooms + bathrooms, data=house) 1-(1-summary(model2)$r.squared)*(length(house$price)-1)/ ((length(house$price)-2-1)) ## [1] 0.3763283 summary(model2)$adj.r.squared ## [1] 0.3763283 7.5 Lab Questions: Congratulations! You’ve been hired as an analyst by the [insert your favorite baseball team name here]. Your first job is to make a few predictions about what your teams’ future will look like. Download lab_vi.Rmd from Canvas Download bbData.RData from Canvas Try the lab questions! 7.5.1 Load in the bbDta.RData file. load(&quot;~/GOVT5001/Lab VI/bbData.RData&quot;) 7.5.2 Run a regression model with attendance as the DV and wins as the IV. Save as model1. ## Simple Regression Model model1 &lt;- lm(home_attend ~ wins, data = bbData) model1 ## ## Call: ## lm(formula = home_attend ~ wins, data = bbData) ## ## Coefficients: ## (Intercept) wins ## -378164 27345 7.5.3 Does winning increase attendance? If so, by how much? ## Pulling out coefficient model1$coefficients[2] ## wins ## 27345.18 7.5.4 Create a scatterplot to illustrate your results from A and B. ## Hint: Run the line of code below to remove scientific notation. options(scipen = 999999) ## Creating Plot ggplot(bbData, aes(x = wins, y = home_attend/1000000)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + labs(title = &quot;Baseball Attendance by Wins&quot;, x = &quot;Wins&quot;, y = &quot;Attendance At Home Games in Millions&quot;) 7.5.5 Does a higher number of wins increase attendance when you include runs_scored and runs_allowed into your model? What is wins’ new effect. Save the results as model2. ## Multiple Regression Model model2 &lt;- lm(home_attend~wins + runs_scored + runs_allowed, data=bbData) model2 ## ## Call: ## lm(formula = home_attend ~ wins + runs_scored + runs_allowed, ## data = bbData) ## ## Coefficients: ## (Intercept) wins runs_scored runs_allowed ## -270324 3261 4260 -1683 Wins increase attendance by 3,261 while holding all other factors equal. 7.5.6 Assess fit. What is the change in R^2 from model1 to model2? ## Examining R Squared summary(model2)$adj.r.squared ## [1] 0.2933172 summary(model2)$adj.r.squared - summary(model1)$r.squared ## [1] 0.08311133 Model 2 explains 29% of the variation in home_attendance. The change is about * percentage points. 7.5.7 Which model do you think is best? For our purposes, model 2 has the highest r squared, so model 2 is the best. 7.5.8 Use your preferred model to predict attendance based on 100 wins, 500 runs scored, and 360 runs allowed. ## Making Predictions -270324 + (3261*100) + (4260*500) + (-1683*360) ## [1] 1579896 ## With Code model2$coefficients[1] + (model2$coefficients[2]*100) + (model2$coefficients[3]*500) + (model2$coefficients[4]*360) ## (Intercept) ## 1580066 7.5.9 Based on your models, how can teams increase their attendance? Answer: Teams should win more games, score more runs, and allow less runs scored against them. Easy! 7.5.10 Your colleague argues that runs have been increasing over time. Create a scatterplot, with a line of best fit, to evaluate their claim. ## Plot ggplot(bbData, aes(x = season, y = runs_scored)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + labs(title = &quot;Runs Scored Over Time&quot;, x = &quot;Season Year&quot;, y = &quot;Runs Scored&quot;) 7.5.11 The analysis above is for the full MLB. Rerun your models and scatterplots for one team. ## Subset the data phiData &lt;- subset(bbData, team == &quot;PHI&quot;) ## Models model1b &lt;- lm(home_attend~wins, data = phiData) model1b ## ## Call: ## lm(formula = home_attend ~ wins, data = phiData) ## ## Coefficients: ## (Intercept) wins ## -293337 29191 model1 ## ## Call: ## lm(formula = home_attend ~ wins, data = bbData) ## ## Coefficients: ## (Intercept) wins ## -378164 27345 model2b &lt;- lm(home_attend~wins + runs_scored + runs_allowed, data = phiData) model2b ## ## Call: ## lm(formula = home_attend ~ wins + runs_scored + runs_allowed, ## data = phiData) ## ## Coefficients: ## (Intercept) wins runs_scored runs_allowed ## 1060314 12020 2783 -2745 model2 ## ## Call: ## lm(formula = home_attend ~ wins + runs_scored + runs_allowed, ## data = bbData) ## ## Coefficients: ## (Intercept) wins runs_scored runs_allowed ## -270324 3261 4260 -1683 ## Wins and Attendance ggplot(bbData, aes(x = wins, y = home_attend/1000000)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + labs(title = &quot;Baseball Attendance by Wins&quot;, x = &quot;Wins&quot;, y = &quot;Attendance At Home Games in Millions&quot;, subtitle = &quot;Philadelphia Phillies&quot;) ## Runs Scored by Season ggplot(phiData, aes(x = season, y = runs_scored)) + geom_line() + theme_bw() + labs(title = &quot;Runs Scored Over Time&quot;, x = &quot;Season Year&quot;, y = &quot;Runs Scored&quot;, subtitle = &quot;Philadelphia Phillies&quot;) ## Line Plot ggplot(phiData, aes(x = season, y = runs_scored)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + labs(title = &quot;Runs Scored Over Time&quot;, x = &quot;Season Year&quot;, y = &quot;Runs Scored&quot;, subtitle = &quot;Philadelphia Phillies&quot;) "],["lab-vii-interactions-categorical-variables.html", "8 Lab VII: Interactions &amp; Categorical Variables 8.1 Categorical Variables and Reference Categories 8.2 Interactions 8.3 Lab Questions", " 8 Lab VII: Interactions &amp; Categorical Variables 8.1 Categorical Variables and Reference Categories 8.1.1 Loading Data load(&quot;~/GOVT5001/Lab VII/social.RData&quot;) 8.1.2 The Four Categories of the Treatment Variable unique(social$messages) ## [1] &quot;Civic Duty&quot; &quot;Hawthorne&quot; &quot;Control&quot; &quot;Neighbors&quot; 8.1.3 Estimating the Model reg1 &lt;- lm(primary2006~messages, data=social) 8.1.4 Difference in Means Between Categories reg1$coefficients[1] ## (Intercept) ## 0.3145377 reg1$coefficients[2] ## messagesControl ## -0.01789934 reg1$coefficients[3] ## messagesHawthorne ## 0.007836968 reg1$coefficients[4] ## messagesNeighbors ## 0.06341057 8.1.5 Changing the Reference Category social$messages &lt;- as.factor(social$messages) reg2 &lt;- lm(primary2006~relevel(messages, &quot;Control&quot;), data=social) reg2$coefficients[1] ## (Intercept) ## 0.2966383 reg2$coefficients[2] ## relevel(messages, &quot;Control&quot;)Civic Duty ## 0.01789934 reg2$coefficients[3] ## relevel(messages, &quot;Control&quot;)Hawthorne ## 0.02573631 reg2$coefficients[4] ## relevel(messages, &quot;Control&quot;)Neighbors ## 0.08130991 8.2 Interactions How to use interactions In R lm(y~x*z, data=data) or lm(y~x + z + x:z, data=data) We write a model with an interaction term as: \\[\\hat{Y_i}=\\hat{\\beta_0} + \\hat{\\beta_1}X_i + \\hat{\\beta_2}Z_i + \\hat{\\beta_3}(X_i*Z_i)\\] 8.2.1 Interpreting Interactions \\[\\hat{Wage_i}=\\hat{\\beta_0} + \\hat{\\beta_1}Experience_i + \\hat{\\beta_2}Male_i + \\hat{\\beta_3}Experience_i*Male_i\\] \\(\\beta_0\\) - Predicted wage for women with no experience \\(\\beta_0 + \\beta_1\\) - Relationship between experience and earnings for women \\(\\beta_0 + \\beta_1 + \\beta_2 + \\beta_3\\) - Predicted Wage for men with experience \\(\\beta_1\\) - The effect of experience for women \\(\\beta_0 + \\beta_2\\) - Predicted earnings for men when experience is zero \\(\\beta_1 + \\beta_3\\) - Relationship between experience and earnings for men 8.3 Lab Questions 8.3.1 What is the effect of age on support for Trump, conditional on being a Republican? Do this with both the colon, “:”, and the “*“. model1a &lt;- lm(TrumpFT~Rep*Age, data=dta) model1a ## ## Call: ## lm(formula = TrumpFT ~ Rep * Age, data = dta) ## ## Coefficients: ## (Intercept) RepTRUE Age RepTRUE:Age ## 13.1949 21.1904 0.0509 0.3780 model1b &lt;- lm(TrumpFT~Rep + Age + Rep:Age, data=dta) model1b ## ## Call: ## lm(formula = TrumpFT ~ Rep + Age + Rep:Age, data = dta) ## ## Coefficients: ## (Intercept) RepTRUE Age RepTRUE:Age ## 13.1949 21.1904 0.0509 0.3780 8.3.2 What is the effect of age on support for Trump, conditional on being a Democrat? model2a &lt;- lm(TrumpFT~Dem*Age, data=dta) model2a ## ## Call: ## lm(formula = TrumpFT ~ Dem * Age, data = dta) ## ## Coefficients: ## (Intercept) DemTRUE Age DemTRUE:Age ## 22.5503 -9.3525 0.5281 -0.5749 model2b &lt;- lm(TrumpFT~Dem + Age + Dem:Age, data=dta) model2b ## ## Call: ## lm(formula = TrumpFT ~ Dem + Age + Dem:Age, data = dta) ## ## Coefficients: ## (Intercept) DemTRUE Age DemTRUE:Age ## 22.5503 -9.3525 0.5281 -0.5749 8.3.3 Interpret the following plot. plot(dta$Age, dta$TrumpFT, color=dta$Rep, col=c(&quot;red&quot;, &quot;blue&quot;), main=&quot;The Effect of Age on Support for Trump Conditional on Party ID&quot;, xlab = &quot;Age&quot;, ylab = &quot;Trump FT&quot;, las=1, xlim=c(0,100), pch=&quot;&quot;) abline(lm(TrumpFT~Age, data = subset(dta, Rep==TRUE)), col=&quot;dark red&quot;, lwd=3) abline(lm(TrumpFT~Age, data = subset(dta, Dem==TRUE)), col=&quot;dark blue&quot;, lwd=3) 8.3.4 What are your predictions for how women may feel about Clinton, how Democrats may feel about Clinton, and how Democratic women may feel about Clinton? Then estimate the model. Most answers will be good here. The idea is just to think about conditional effects. For example, women may approve more of Clinton because she could have been the first woman president. Democrats probably approve of Clinton because she was the Democratic nominee for president. Democratic women, then, may support Clinton EVEN more than Republican women and Democratic men. The conditional effect of being a Democrat for women will be positive. model3 &lt;- lm(ClintonFT~Female*Dem, data=dta) model3 ## ## Call: ## lm(formula = ClintonFT ~ Female * Dem, data = dta) ## ## Coefficients: ## (Intercept) Female DemTRUE Female:DemTRUE ## 17.690 1.114 27.621 6.882 8.3.5 How do we interpret the following: Intercept = Clinton Feelings for Male Republicans Intercept + Beta 1 = Clinton Feelings for Female Republicans Intercept + Beta 2 = Clinton Feelings for Male Democrats Intercept + Beta 1 + Beta 2 + Beta 3 = Clinton Feelings for Female Democrats Beta 1 - The Effect of Sex for Republicans Beta 2 - The Effect of Party for Men Beta 2 + Beta 3 - The Effect of Party for Women Beta 1 + Beta 3 - The Effect of Sex for Democrats 8.3.6 Estimate the the same model as above, but for Trump and Republicans. model4 &lt;- lm(TrumpFT~Female*Rep, data=dta) model4 ## ## Call: ## lm(formula = TrumpFT ~ Female * Rep, data = dta) ## ## Coefficients: ## (Intercept) Female RepTRUE Female:RepTRUE ## 19.134 -8.887 33.196 5.965 8.3.7 Calculate the following: What is Trump’s average feeling for male Democrats? - 19.134 What is Trump’s average feeling for female Democrats? - 10.247 What is Trump’s average feeling for male Republicans? - 52.33 What is Trump’s average feeling for female Republicans? - 49.408 "],["lab-viii-publication-ready-tables-interactions-cont..html", "9 Lab VIII: Publication Ready Tables &amp; Interactions (cont.) 9.1 Making Tables 9.2 Interactions Practice Problems 9.3 Appendix: Additional Materials", " 9 Lab VIII: Publication Ready Tables &amp; Interactions (cont.) 9.1 Making Tables ## Loading Packages library(stargazer) ## Loading Data - Change this to your working directory load(&quot;~/GOVT5001/Lab VI/bbData.RData&quot;) ## Running Models model1 &lt;- lm(home_attend ~ wins, data = bbData) model2 &lt;- lm(home_attend ~ wins + runs_scored + runs_allowed, data=bbData) Dependent variable: home_attend (1) (2) wins 27,345.180*** 3,261.208 (1,833.210) (3,364.526) runs_scored 4,260.215*** (489.056) runs_allowed -1,682.885*** (378.041) Constant -378,163.700*** -270,323.800 (146,400.900) (237,057.800) Observations 838 838 R2 0.210 0.296 Adjusted R2 0.209 0.293 Residual Std. Error 672,148.600 (df = 836) 635,420.200 (df = 834) F Statistic 222.504*** (df = 1; 836) 116.802*** (df = 3; 834) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 stargazer(model1, model2, covariate.labels = c(&quot;Wins&quot;, &quot;Runs Scored&quot;, &quot;Runs Allowed&quot;), dep.var.labels = &quot;Home Attendance&quot;, header = FALSE, title = &quot;OLS Results&quot;, type = &quot;html&quot;) OLS Results Dependent variable: Home Attendance (1) (2) Wins 27,345.180*** 3,261.208 (1,833.210) (3,364.526) Runs Scored 4,260.215*** (489.056) Runs Allowed -1,682.885*** (378.041) Constant -378,163.700*** -270,323.800 (146,400.900) (237,057.800) Observations 838 838 R2 0.210 0.296 Adjusted R2 0.209 0.293 Residual Std. Error 672,148.600 (df = 836) 635,420.200 (df = 834) F Statistic 222.504*** (df = 1; 836) 116.802*** (df = 3; 834) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 stargazer(model1, model2, covariate.labels = c(&quot;Wins&quot;, &quot;Runs Scored&quot;, &quot;Runs Allowed&quot;), dep.var.labels = &quot;Home Attendance&quot;, header = FALSE, title = &quot;OLS Results&quot;, omit.stat = c(&quot;rsq&quot;, &quot;adj.rsq&quot;), column.labels = c(&quot;Model 1&quot;, &quot;Model 2&quot;), type = &quot;html&quot;) OLS Results Dependent variable: Home Attendance Model 1 Model 2 (1) (2) Wins 27,345.180*** 3,261.208 (1,833.210) (3,364.526) Runs Scored 4,260.215*** (489.056) Runs Allowed -1,682.885*** (378.041) Constant -378,163.700*** -270,323.800 (146,400.900) (237,057.800) Observations 838 838 Residual Std. Error 672,148.600 (df = 836) 635,420.200 (df = 834) F Statistic 222.504*** (df = 1; 836) 116.802*** (df = 3; 834) Note: p&lt;0.1; p&lt;0.05; p&lt;0.01 9.2 Interactions Practice Problems These lab questions will test both your programming and interpretation skills! It covers most of the programming and regression related topics we have covered up until now. We will be exploring the relationship between politics and monetary policy in the US. 9.2.1 In your own words, what is an interaction term? An interaction occurs when an independent variable has a different effect on the outcome depending on the values of another independent variable. We can think of this as a conditional effect. 9.2.2 What is the Federal Reserve Funds Rate? The interest rate that banks charge each other to borrow or lend excess reserves overnight. It is “set” by the Federal Open Markets Committee. A higher Fed Funds Rate means more expensive borrowing costs. Banks can then pass on higher borrowing costs by raising the rates they charge for consumer loans. 9.2.3 What is the Federal Reserves’ Dual Mandate? The dual mandate is usually discussed as keeping unemployment and inflation at normative rates. About 1-2% for inflation and 4-5% for unemployment. 9.2.4 Estimate a model with FEDFUNDS as the DV, Democrat and Quarters as the IVs, and include an interaction between Democrat and Quarters. ## Loading Data load(&quot;~/GOVT8001/Lab 8/fed.RData&quot;) model1 &lt;- lm(FEDFUNDS~Democrat + Quarters + Democrat:Quarters, data=dta) model1 ## ## Call: ## lm(formula = FEDFUNDS ~ Democrat + Quarters + Democrat:Quarters, ## data = dta) ## ## Coefficients: ## (Intercept) Democrat Quarters Democrat:Quarters ## 7.7703 -4.9032 -0.2649 0.5582 9.2.5 You can also create an “intereacted” variable and include it in the model. Re-estimate the model above with a new variable called dem_quarters that is the interaction term between quarters and Democrat. dta$dem_quarters &lt;- dta$Democrat*dta$Quarters model2 &lt;- lm(FEDFUNDS~ Democrat + Quarters + dem_quarters, data=dta) model2 ## ## Call: ## lm(formula = FEDFUNDS ~ Democrat + Quarters + dem_quarters, data = dta) ## ## Coefficients: ## (Intercept) Democrat Quarters dem_quarters ## 7.7703 -4.9032 -0.2649 0.5582 9.2.6 What change in federal fund rates is associated with a one-unit increase in the quarters variable when the president is a Republican? A one-unit increase in the election variable when the president is a Republican is associated with a fall in the Federal Funds rate of 0.26 percentage points. It is simply the coefficient on the quarters variable. 9.2.7 What change in federal fund rates is associated with a one-unit increase in the election variable when the president is a Democrat? A one-unit increase in the election variable when the president is a Democrat is associated with an increase in the Federal Funds rate of -0.264 + 0.558 = 0.293 percentage points. The effect when a Democrat is president is the sum of the coefficients on the election variable and the interaction of the election variable and the dummy for Democratic presidents. 9.2.8 Create a Republican variable and estimate the same model as above, but for Republicans. dta &lt;- dta %&gt;% mutate(Republican = if_else(dta$Democrat == 0, 1, 0)) model2b &lt;- lm(FEDFUNDS~Republican*Quarters, data=dta) model2b ## ## Call: ## lm(formula = FEDFUNDS ~ Republican * Quarters, data = dta) ## ## Coefficients: ## (Intercept) Republican Quarters Republican:Quarters ## 2.8671 4.9032 0.2933 -0.5582 9.2.9 Create two scatterplots, for years in which a Democrat was president and one for years in which a Republican was president, showing the relationship between inflation and the quarters since the previous election. Repeat this for the Fed Funds Rate. ## Plot for Democrats dta %&gt;% filter(Democrat == 1) %&gt;% ggplot(aes(x = Quarters, y = inflation)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;blue&quot;) + theme_bw() + labs(title = &quot;Inflation When Democrat is in Office&quot;, x = &quot;Quarters from Last Election&quot;, y = &quot;Inflation&quot;) ## Plot for Republicans dta %&gt;% filter(Democrat == 0) %&gt;% ggplot(aes(x = Quarters, y = inflation)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) + theme_bw() + labs(title = &quot;Inflation When Republican is in Office&quot;, x = &quot;Quarters from Last Election&quot;, y = &quot;Inflation&quot;) ## Plot for Democrats dta %&gt;% filter(Democrat == 1) %&gt;% ggplot(aes(x = Quarters, y = FEDFUNDS)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;blue&quot;) + theme_bw() + labs(title = &quot;Fed Funds Rate When Democrat is in Office&quot;, x = &quot;Quarters from Last Election&quot;, y = &quot;Fed Funds Rate&quot;) ## Plot for Republicans dta %&gt;% filter(Democrat == 0) %&gt;% ggplot(aes(x = Quarters, y = FEDFUNDS)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F, col = &quot;red&quot;) + theme_bw() + labs(title = &quot;Fed Funds Rate When Republican is in Office&quot;, x = &quot;Quarters from Last Election&quot;, y = &quot;Fed Funds Rate&quot;) 9.2.10 How responsive is the Fed to rising inflation? Present a scatterplot with a regression line and calculate the correlation coefficient and estimate a simple regression model. ## Plot dta %&gt;% ggplot(aes(x = inflation, y = FEDFUNDS)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + labs(title = &quot;Fed Funds Rate &amp; Inflation&quot;, x = &quot;Inflation&quot;, y = &quot;Fed Funds Rate&quot;) ## Correlation Coefficient cor(dta$inflation, dta$FEDFUNDS, use=&quot;complete.obs&quot;) ## [1] 0.6951576 ## Regression Model summary(lm(FEDFUNDS ~ inflation, data = dta)) ## ## Call: ## lm(formula = FEDFUNDS ~ inflation, data = dta) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3947 -1.4348 -0.2644 1.0830 8.9465 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.58411 0.25541 10.12 &lt;0.0000000000000002 *** ## inflation 0.75964 0.05249 14.47 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.453 on 224 degrees of freedom ## (6 observations deleted due to missingness) ## Multiple R-squared: 0.4832, Adjusted R-squared: 0.4809 ## F-statistic: 209.5 on 1 and 224 DF, p-value: &lt; 0.00000000000000022 9.2.11 Let’s find the average inflation rate for each presidential administration in our dataset. Start by creating a new categorical variable with the name of each president. Hint 1: run dta$president to see how the dataset codes presidents. Hint 2: Look up case_when(). ## With dplyr dta &lt;- dta %&gt;% mutate(admin = case_when( president == 1 ~ &quot;Obama&quot;, president == 2 | president == 3 ~ &quot;Eisenhower&quot;, president == 4 ~ &quot;H.W.&quot;, president == 5 | president == 6 ~ &quot;Bush&quot;, president == 7 ~ &quot;Carter&quot;, president == 8 ~ &quot;Kennedy-Johnson&quot;, president == 9 ~ &quot;Johnson&quot;, president == 10 ~ &quot;Nixon-Ford&quot;, president == 11 ~ &quot;Nixon&quot;, president == 12 | president == 13 ~ &quot;Reagan&quot;, president == 14 | president == 15 ~ &quot;Clinton&quot;) ) ## With Base R dta$admin2 &lt;- NA dta$admin2[dta$president==1] &lt;- &quot;Obama&quot; dta$admin2[dta$president==2 | dta$president==3] &lt;- &quot;Eisenhower&quot; dta$admin2[dta$president==4] &lt;- &quot;H.W.&quot; dta$admin2[dta$president==5 | dta$president==6] &lt;- &quot;Bush&quot; dta$admin2[dta$president==7] &lt;- &quot;Carter&quot; dta$admin2[dta$president==8] &lt;- &quot;Kennedy-Johnson&quot; dta$admin2[dta$president==9] &lt;- &quot;Johnson&quot; dta$admin2[dta$president==10] &lt;- &quot;Nixon-Ford&quot; dta$admin2[dta$president==11] &lt;- &quot;Nixon&quot; dta$admin2[dta$president==12 | dta$president==13] &lt;- &quot;Reagan&quot; dta$admin2[dta$president==14 | dta$president==15] &lt;- &quot;Clinton&quot; 9.2.12 Now find the average inflation rate for each administration. ## WIth dplyr dta %&gt;% group_by(admin) %&gt;% summarise(avg_inflation = mean(inflation)) ## # A tibble: 11 × 2 ## admin avg_inflation ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bush 2.57 ## 2 Carter 9.79 ## 3 Clinton 2.56 ## 4 Eisenhower 1.37 ## 5 H.W. 4.18 ## 6 Johnson 3.20 ## 7 Kennedy-Johnson 1.16 ## 8 Nixon 4.50 ## 9 Nixon-Ford 8.00 ## 10 Obama 1.32 ## 11 Reagan 4.29 ## With Base R tapply(dta$inflation, dta$admin, mean) ## Bush Carter Clinton Eisenhower H.W. Johnson Kennedy-Johnson ## 2.573187 9.792505 2.563147 1.371580 4.176468 3.201017 1.161032 ## Nixon Nixon-Ford Obama Reagan ## 4.495170 7.996118 1.324935 4.289426 9.2.13 Problem IV With any time left, we will go over last week’s questions! 9.3 Appendix: Additional Materials ## Running Model model2 &lt;- lm(home_attend ~ wins + runs_scored + runs_allowed, data=bbData) ## Packages for Partial Effects Plot library(ggeffects) library(sjPlot) library(ggplot2) ## Function for Partial Effects plot_model(model2, type = &quot;pred&quot;, terms = &quot;wins&quot;) + theme_bw() ## Loading Federal Reserve Data load(&quot;~/GOVT8001/Lab 8/fed.RData&quot;) ## Plotting Interactions dta %&gt;% ggplot(aes(x = Quarters, y = FEDFUNDS, color = as.character(Democrat), group= Democrat)) + geom_smooth(method = &quot;lm&quot;, se = F) + theme_bw() + ylim(4.5, 6) + labs(title = &quot;Fed Funds Rate When Democrat is in Office&quot;, x = &quot;Quarters from Last Election&quot;, y = &quot;Fed Funds Rate&quot;, color = &quot;Democrat&quot;) "]]

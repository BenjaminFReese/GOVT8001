[["index.html", "GOVT 8001: Shared Working Book 1 Introduction 1.1 Purpose of This Site 1.2 Loading Data", " GOVT 8001: Shared Working Book Benjamin Reese 1 Introduction 1.1 Purpose of This Site The goal of this site is to have a place where you can look for examples of code. Think of this site as a log of all of the topics we cover in class and in lab sessions. I will make updates throughout the semester as we cover more advanced material. Let me know if any of this code doesn’t run correctly or if you have any questions or issues! 1.2 Loading Data The easiest way to load data into R and ensure you have the correct file path is to create a folder on your computer for each assignment and place the datasets directly into that folder. Create a folder on your computer for each new analysis Download your Data and move the file to your newly created folder Then open RStudio Click the project button in the top right corner Click new project Click existing directory Click browse and find the folder that you created Click create project Once your new project opens, click the blank page with a green plus sign icon in the top left corner under the file option Click R script to open a new script You should also be able to see your data file in the bottom right window of RStudio, click the file and follow the options depending on the file type Once your data is imported into R, the code that R automatically ran will be in the console window on the bottom left, copy and paste it to your fresh R script For example, in Lab 1, my code looked like: read_excel(\"Data/USstates.xlsx\") Run this copy and pasted line of code whenever you open the R Project and you will never have to worry about complicated file pathing commands I recommend using the assignment operator &lt;- to give your dataset a short and simple name like df, dta, or, if you are working with multiple datasets, name each something short and descriptive. "],["lab-i-introduction-to-r-r-studio.html", "2 Lab I: Introduction to R &amp; R Studio 2.1 Intro to R 2.2 Intro to library(tidyverse)", " 2 Lab I: Introduction to R &amp; R Studio 2.1 Intro to R # Author: QSS Ch. 1 script with edits by Mark Richardson &amp; Benjamin Reese # Date: 08/24/2023 # Purpose: Introduction to R - GOVT 8001 Lab I #### Arithmetic Operations #### 5 + 3 ## [1] 8 5 - 3 ## [1] 2 5 / 3 ## [1] 1.666667 5 ^ 3 ## [1] 125 5 * (10 - 3) ## [1] 35 sqrt(4) ## [1] 2 #### Objects #### result &lt;- 5 + 3 result ## [1] 8 print(result) ## [1] 8 result &lt;- 5 - 3 result ## [1] 2 ## R is case sensitive so we get an error. Result kosuke &lt;- &quot;instructor&quot; kosuke ## [1] &quot;instructor&quot; kosuke &lt;- &quot;instructor and author&quot; kosuke ## [1] &quot;instructor and author&quot; Result &lt;- 5 Result + 2 ## [1] 7 result ## [1] 2 class(result) ## [1] &quot;numeric&quot; Result ## [1] 5 class(Result) ## [1] &quot;numeric&quot; class(sqrt) ## [1] &quot;function&quot; sum(result) ## [1] 2 sum(Result) ## [1] 5 #### Vectors #### # Creating vectors world.pop &lt;- c(2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183) world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 pop.first &lt;- c(2525779, 3026003, 3691173) pop.second &lt;- c(4449049, 5320817, 6127700, 6916183) pop.all &lt;- c(pop.first, pop.second) pop.all ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 # Accessing elements of a vector world.pop[2] ## [1] 3026003 world.pop[c(2, 4)] ## [1] 3026003 4449049 world.pop[c(4, 2)] ## [1] 4449049 3026003 world.pop[-3] ## [1] 2525779 3026003 4449049 5320817 6127700 6916183 # Arithmetic operations on a vector pop.million &lt;- world.pop / 1000 pop.million ## [1] 2525.779 3026.003 3691.173 4449.049 5320.817 6127.700 6916.183 pop.rate &lt;- world.pop / world.pop[1] pop.rate ## [1] 1.000000 1.198047 1.461400 1.761456 2.106604 2.426063 2.738238 pop.increase &lt;- world.pop[-1] - world.pop[-7] pop.increase ## [1] 500224 665170 757876 871768 806883 788483 percent.increase &lt;- (pop.increase / world.pop[-7]) * 100 percent.increase ## [1] 19.80474 21.98180 20.53212 19.59448 15.16464 12.86752 # Can replace individual elements (better way is to use round()) round(percent.increase) ## [1] 20 22 21 20 15 13 percent.increase[c(1, 2)] &lt;- c(20, 22) percent.increase ## [1] 20.00000 22.00000 20.53212 19.59448 15.16464 12.86752 #### Functions #### length(world.pop) ## [1] 7 min(world.pop) ## [1] 2525779 max(world.pop) ## [1] 6916183 range(world.pop) ## [1] 2525779 6916183 mean(world.pop) ## [1] 4579529 sum(world.pop) / length(world.pop) ## [1] 4579529 year &lt;- seq(from = 1950, to = 2010, by = 10) year ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(to = 2010, by = 10, from = 1950) ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(1950, 2010, 10) ## [1] 1950 1960 1970 1980 1990 2000 2010 seq(2010, 1950, -10) ## [1] 2010 2000 1990 1980 1970 1960 1950 seq(from = 2010, to = 1950, by = -10) ## [1] 2010 2000 1990 1980 1970 1960 1950 2008:2012 ## [1] 2008 2009 2010 2011 2012 2012:2008 ## [1] 2012 2011 2010 2009 2008 names(world.pop) ## NULL names(world.pop) &lt;- year names(world.pop) ## [1] &quot;1950&quot; &quot;1960&quot; &quot;1970&quot; &quot;1980&quot; &quot;1990&quot; &quot;2000&quot; &quot;2010&quot; world.pop ## 1950 1960 1970 1980 1990 2000 2010 ## 2525779 3026003 3691173 4449049 5320817 6127700 6916183 #### Saving data and loading data #### # Create a data set (Table 1.2) # tibble() is the equivalent of data.frame() tidyverse function from the tibble package UNpop &lt;- data.frame(world.pop = world.pop, year = year) # Get basic information about the data set names(UNpop) ## [1] &quot;world.pop&quot; &quot;year&quot; nrow(UNpop) ## [1] 7 ncol(UNpop) ## [1] 2 dim(UNpop) ## [1] 7 2 summary(UNpop) ## world.pop year ## Min. :2525779 Min. :1950 ## 1st Qu.:3358588 1st Qu.:1965 ## Median :4449049 Median :1980 ## Mean :4579529 Mean :1980 ## 3rd Qu.:5724258 3rd Qu.:1995 ## Max. :6916183 Max. :2010 UNpop$world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 UNpop[, &quot;world.pop&quot;] # extract the column called &quot;world.pop&quot; ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 UNpop[c(1, 2, 3, 5), ] # extract the first three rows (and all columns) ## world.pop year ## 1950 2525779 1950 ## 1960 3026003 1960 ## 1970 3691173 1970 ## 1990 5320817 1990 UNpop[1:3, &quot;year&quot;] # extract the first three rows of the &quot;year&quot; column ## [1] 1950 1960 1970 UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)] ## [1] 2525779 3691173 5320817 6916183 # File paths and working directory getwd() # Confirm the change ## [1] &quot;C:/Users/17176/Documents/GOVT701&quot; #### Getting Help: mean() example #### world.pop &lt;- c(UNpop$world.pop, NA) world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 NA mean(world.pop) ## [1] NA ## Use Question Marks to see help documentation ?mean mean(world.pop, na.rm = TRUE) ## [1] 4579529 2.2 Intro to library(tidyverse) # Packages ## install.packages(&quot;devtools&quot;) # install the package library(devtools) # load the package ## install a package from github ## devtools::install_github(&quot;kosukeimai/qss-package&quot;, build_vignettes = TRUE) library(qss) ## loading in qss ## You may need to allow R to update/install additional packages ## Loading in tidyverse ## install.packages(&quot;tidyverse&quot;) library(tidyverse) ## Loading in a Dataset data(UNpop, package = &quot;qss&quot;) ## Number of Rows and Columns - Base R dim(UNpop) ## [1] 7 2 ## Number of observation, number of variables, and initial observations - tidyverse glimpse(UNpop) ## Rows: 7 ## Columns: 2 ## $ year &lt;int&gt; 1950, 1960, 1970, 1980, 1990, 2000, 2010 ## $ world.pop &lt;int&gt; 2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183 ## First 6 rows head(UNpop) ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## 4 1980 4449049 ## 5 1990 5320817 ## 6 2000 6127700 ## Last 6 Rows tail(UNpop) ## year world.pop ## 2 1960 3026003 ## 3 1970 3691173 ## 4 1980 4449049 ## 5 1990 5320817 ## 6 2000 6127700 ## 7 2010 6916183 ## Selecting A Variable - Base R UNpop$world.pop ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 ## subset all rows for the column called &quot;world.pop&quot; from the UNpop data UNpop[, &quot;world.pop&quot;] ## [1] 2525779 3026003 3691173 4449049 5320817 6127700 6916183 ## subset the first three rows (and all columns) UNpop[c(1, 2, 3),] ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## subset the first three rows of the &quot;year&quot; column UNpop[1:3, &quot;year&quot;] ## [1] 1950 1960 1970 ## Now with tidyverse ## Subset the first three rows of UNpop with tidyverse slice(UNpop, 1:3) ## year world.pop ## 1 1950 2525779 ## 2 1960 3026003 ## 3 1970 3691173 ## Extract/subset the world.pop variable (column) select(UNpop, world.pop) ## world.pop ## 1 2525779 ## 2 3026003 ## 3 3691173 ## 4 4449049 ## 5 5320817 ## 6 6127700 ## 7 6916183 ## Base R subset the first three rows of the year variable UNpop[1:3, &quot;year&quot;] ## [1] 1950 1960 1970 ## or in tidyverse, combining slice() and select() select(slice(UNpop, 1:3), year) ## year ## 1 1950 ## 2 1960 ## 3 1970 ## Basic Data Wrangling with the tidyverse using pipes (i.e., %&gt;%) UNpop %&gt;% # take the UNpop data we have loaded, and then... slice(1:3) %&gt;% # subset the first three rows, and then... select(year) # subset the year column ## year ## 1 1950 ## 2 1960 ## 3 1970 UNpop %&gt;% slice(seq(1, n(), by = 2)) %&gt;% # using a sequence from 1 to n() select(world.pop) ## world.pop ## 1 2525779 ## 2 3691173 ## 3 5320817 ## 4 6916183 pop.1970 &lt;- UNpop %&gt;% # take the UNpop data and then.... filter(year == 1970) %&gt;% # subset rows where the year variable is equal to 1970 select(world.pop) %&gt;% # subset just the world.pop column pull() # return a vector, not a tibble ## Print the vector to the console to see it print(pop.1970) ## [1] 3691173 UNpop.mill &lt;- UNpop %&gt;% # create a new tibble from UNpop mutate(world.pop.mill = world.pop / 1000) %&gt;% # create a new variable, world.pop.mill select(-world.pop) # drop the original world.pop column ## Adding a variable with if_else UNpop.mill &lt;- UNpop.mill %&gt;% mutate(after.1980 = if_else(year &gt;= 1980, 1, 0)) ## Creating a vector of the years of interest specific.years &lt;- c(1950, 1980, 2000) ## Adding a variable with if_else and %in% UNpop.mill &lt;- UNpop.mill %&gt;% mutate(year.of.interest = if_else(year %in% specific.years, 1, 0)) summary(UNpop.mill) ## year world.pop.mill after.1980 year.of.interest ## Min. :1950 Min. :2526 Min. :0.0000 Min. :0.0000 ## 1st Qu.:1965 1st Qu.:3359 1st Qu.:0.0000 1st Qu.:0.0000 ## Median :1980 Median :4449 Median :1.0000 Median :0.0000 ## Mean :1980 Mean :4580 Mean :0.5714 Mean :0.4286 ## 3rd Qu.:1995 3rd Qu.:5724 3rd Qu.:1.0000 3rd Qu.:1.0000 ## Max. :2010 Max. :6916 Max. :1.0000 Max. :1.0000 mean(UNpop.mill$world.pop.mill) ## [1] 4579.529 ## Add a row where values for all columns is NA UNpop.mill.wNAs &lt;- UNpop.mill %&gt;% add_row(year = NA, world.pop.mill = NA, after.1980 = NA, year.of.interest = NA) ## Take the mean of world.pop.mill (returns NA) mean(UNpop.mill.wNAs$world.pop.mill) ## [1] NA ## Take the mean of world.pop.mill (ignores the NA) mean(UNpop.mill.wNAs$world.pop.mill, na.rm = TRUE) ## [1] 4579.529 ## Other Summary Statistics with tidyverse UNpop.mill %&gt;% summarize(mean.pop = mean(world.pop.mill), median.pop = median(world.pop.mill)) ## mean.pop median.pop ## 1 4579.529 4449.049 UNpop.mill %&gt;% group_by(after.1980) %&gt;% # create subset group for each value of after.1980 summarize(mean.pop = mean(world.pop.mill)) # calculate mean for each group ## # A tibble: 2 × 2 ## after.1980 mean.pop ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3081. ## 2 1 5703. "],["lab-ii-introduction-to-librarytidyverse-r-markdown.html", "3 Lab II: Introduction to library(tidyverse) &amp; R Markdown 3.1 In the setup chunk above, load the tidyverse packages as well as library(readr) 3.2 Load in the resume.RData file and use head(), tail(), glimpse(), dim(), summary(), and View() to examine each variable in the dataset. How many of the resumes have white sounding names? How many have African-American sounding names. 3.3 This experiment seeks to determine whether or not hiring managers discriminate on the basis of racial identity by sending idential resumes with African-American and white sounding names to job postings. The basic logic is that resumes are identical and only the name is changing, so any differences in call backs for jobs can be attributed to racial discrimination. Why do the authors want to randomize? And do you think this is an effective research design? 3.4 We are going to see if there is a racial discrepency by taking the difference in callback rates between racial groups. Calculate the callback rate for white sounding name applicants and African-American sounding name applicants. Use Latex commands to write the formula for this calculation and display the result in text. Write the formula between $’s like \\(y = mx + b\\) to use Latex commands. 3.5 Now, create a new object that stores the difference in callback rates named race_diff. 3.6 Since Crenshaw (1989), manny scholars have concerned with intersectionality, or how race and gender interact to make the experiences of African-American women unique. We can use the data we have to explore the effect of race and gender specific sounding names on employment prospects. Calculate the call back rate by each race and gender category. 3.7 What is the difference in call back rates for each race/gender group?", " 3 Lab II: Introduction to library(tidyverse) &amp; R Markdown We can use R Markdown to create well-formatted PDFs or .html files that can easily display the results of our analyses. R Markdown, through Latex, also allows to write mathematical formulas with ease. Go ahead and knit - found in the top left corner - this file now and see what it looks like. 3.1 In the setup chunk above, load the tidyverse packages as well as library(readr) ## Example Setup Chunk knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE) ## Packages library(readr) library(tidyverse) 3.2 Load in the resume.RData file and use head(), tail(), glimpse(), dim(), summary(), and View() to examine each variable in the dataset. How many of the resumes have white sounding names? How many have African-American sounding names. ## Loading Data data(resume, package = &quot;qss&quot;) ## Learning About the Dataset head(resume) ## firstname sex race call ## 1 Allison female white 0 ## 2 Kristen female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 ## 5 Carrie female white 0 ## 6 Jay male white 0 tail(resume) ## firstname sex race call ## 4865 Lakisha female black 0 ## 4866 Tamika female black 0 ## 4867 Ebony female black 0 ## 4868 Jay male white 0 ## 4869 Latonya female black 0 ## 4870 Laurie female white 0 glimpse(resume) ## Rows: 4,870 ## Columns: 4 ## $ firstname &lt;chr&gt; &quot;Allison&quot;, &quot;Kristen&quot;, &quot;Lakisha&quot;, &quot;Latonya&quot;, &quot;Carrie&quot;, &quot;Jay&quot;, &quot;Jill&quot;, &quot;Kenya&quot;, &quot;Latonya&quot;, &quot;Tyrone&quot;, … ## $ sex &lt;chr&gt; &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;fe… ## $ race &lt;chr&gt; &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;white&quot;, &quot;white&quot;, &quot;white&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, &quot;black&quot;, … ## $ call &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … dim(resume) ## [1] 4870 4 summary(resume) ## firstname sex race call ## Length:4870 Length:4870 Length:4870 Min. :0.00000 ## Class :character Class :character Class :character 1st Qu.:0.00000 ## Mode :character Mode :character Mode :character Median :0.00000 ## Mean :0.08049 ## 3rd Qu.:0.00000 ## Max. :1.00000 View(resume) ## Comment this out when knitting. ## Number of observations by race resume %&gt;% group_by(race) %&gt;% count() ## # A tibble: 2 × 2 ## # Groups: race [2] ## race n ## &lt;chr&gt; &lt;int&gt; ## 1 black 2435 ## 2 white 2435 3.3 This experiment seeks to determine whether or not hiring managers discriminate on the basis of racial identity by sending idential resumes with African-American and white sounding names to job postings. The basic logic is that resumes are identical and only the name is changing, so any differences in call backs for jobs can be attributed to racial discrimination. Why do the authors want to randomize? And do you think this is an effective research design? The concerns of examining race and the number of callbacks without randomization is that there could be confounders like workplace connections, amount of education, and employment history that could be correlated with race. It is possible that African-American applicants did not have the same employment and educational opportunities as white Americans, and, therefore, their resumes may look significantly different. This raises issues of counfounding and makes it impossible to differentiate if an employer made their decision based on race or based on the substance of the resume. The authors, though, randomized race, reducing this risk of confounding. Employers in this study are seeing nearly identical resumes, with only the race of the applicant being different, as indicated by a name. The field experiment presented here relies on racial connotations of different names, not explicit racial cues. Therefore, hiring managers are determining an applicant’s race largely based on what scholars of identity politics would call “perceived race” or “street race” (Lopez et al. 2017), which is how others perceive an individual’s race. This fact means that the selection of names is integral to the internal validity of the research design. 3.4 We are going to see if there is a racial discrepency by taking the difference in callback rates between racial groups. Calculate the callback rate for white sounding name applicants and African-American sounding name applicants. Use Latex commands to write the formula for this calculation and display the result in text. Write the formula between $’s like \\(y = mx + b\\) to use Latex commands. ## Call Back for white Sounding Name Applicants resume %&gt;% group_by(race) %&gt;% summarise(callback_rates = mean(call)) ## # A tibble: 2 × 2 ## race callback_rates ## &lt;chr&gt; &lt;dbl&gt; ## 1 black 0.0645 ## 2 white 0.0965 The callback rate for whites is .096. We take the mean of the binary callback variable, \\(\\overline{x} = \\frac{1}{n}\\Sigma^{n}_{i=1}x_i\\) The callback rate for African-American sounding name applicants is .064. 3.5 Now, create a new object that stores the difference in callback rates named race_diff. ## Calculating Callback Proportions race_call &lt;- resume %&gt;% group_by(race, call) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## Difference in call back rates race_diff &lt;- race_call %&gt;% select(race, call_prop) %&gt;% pivot_wider(names_from = c(race), values_from = call_prop) %&gt;% mutate(race_diff = white - black) %&gt;% select(race_diff) ## Printing race_diff ## # A tibble: 1 × 1 ## race_diff ## &lt;dbl&gt; ## 1 0.0320 3.6 Since Crenshaw (1989), manny scholars have concerned with intersectionality, or how race and gender interact to make the experiences of African-American women unique. We can use the data we have to explore the effect of race and gender specific sounding names on employment prospects. Calculate the call back rate by each race and gender category. ## Callbacks by race and gender resume %&gt;% group_by(race, call, sex) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## # A tibble: 4 × 6 ## # Groups: race, sex [4] ## race sex no_call call total_resumes call_prop ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 black female 1761 125 1886 0.0663 ## 2 black male 517 32 549 0.0583 ## 3 white female 1676 184 1860 0.0989 ## 4 white male 524 51 575 0.0887 3.7 What is the difference in call back rates for each race/gender group? ## Saving tibble from 8 dta &lt;- resume %&gt;% group_by(race, call, sex) %&gt;% count() %&gt;% pivot_wider(names_from = call, values_from = n) %&gt;% rename(no_call = `0`, call = `1`) %&gt;% mutate(total_resumes = no_call + call, call_prop = call / total_resumes) ## Calculating Differences call_backs &lt;- dta %&gt;% select(race, sex, call_prop) %&gt;% pivot_wider(names_from = c(sex, race), values_from = call_prop) %&gt;% mutate(white_sex_diff = female_white - male_white, black_sex_diff = female_black - male_black, male_race_diff = male_white - male_black, female_race_diff = female_white - female_black) %&gt;% select(white_sex_diff, black_sex_diff, male_race_diff, female_race_diff) ## Printing print(call_backs) ## print() is optional ## # A tibble: 1 × 4 ## white_sex_diff black_sex_diff male_race_diff female_race_diff ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0102 0.00799 0.0304 0.0326 "],["lab-iii-univariate-visualizations.html", "4 Lab III: Univariate Visualizations 4.1 Histograms with library(ggplot2) 4.2 Barplots with library(ggplot2) 4.3 Now Make Your Own Histogram or Barplot!", " 4 Lab III: Univariate Visualizations ## Packages library(tidyverse) library(ggthemes) ## Data Loading ## Replace this with your working directory load(&quot;~/GOVT8001/Lab 3/white_minwage.RData&quot;) This lab shows step-by-step how to build basic histograms and barplots with library(ggplot2) 4.1 Histograms with library(ggplot2) Histograms are good to visualize the distribution of one continuous variable. 4.1.1 Step One Specify the tibble to be piped into ggplot() ## Building A Basic Histogram df.county 4.1.2 Step Two Pipe the tibble into ggplot() Specify the variable of interest with ggplot(aes(x = X)) ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) 4.1.3 Step 3 Use + instead of %&gt;% to move to next line in ggplot() geom_histogram() creates the histogram ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..)) 4.1.4 Step 4 Customization of theme, colors, and labels. You can also save the object above and customize it later as shown below Use col = and fill = in geom_histogram() to set colors ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) Use + theme() to set the theme library(ggtheme) has themes from your favorite publications! ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) + theme_minimal() Use + labs to set labels title = for a title subtitle = for a subtitle x = for x axis label and y = for y axis label caption = for caption to include data source or note ## Building A Basic Histogram df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) + theme_minimal() + labs(title = &quot;Distribution of Minimum Wage&quot;, subtitle = &quot;All US Counties 1996 - 2016&quot;, x = &quot;Minimum Wage&quot;, caption = &quot;Data Source: Markovich &amp; White (2022)&quot;, y = &quot;Density&quot;) ## Or You Can Save the Basic Plot and Experiment p &lt;- df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;dark red&quot;, fill = &quot;tomato&quot;) p p + theme_minimal() + labs(title = &quot;Distribution of Minimum Wage&quot;, subtitle = &quot;All US Counties 1996 - 2016&quot;, x = &quot;Minimum Wage&quot;, caption = &quot;Data Source: Markovich &amp; White (2022)&quot;, y = &quot;Density&quot;) 4.2 Barplots with library(ggplot2) Barplots are good for visualizing distributions by groups. The steps here follow closely what we did for the histogram. 4.2.1 Step One We will be using simulated data for this example. First we need to format our simulated data into something we can use for the barplot with the skills we learned last week. ## Simulated Data df &lt;- data.frame(&quot;age&quot; = c(&quot;18 to 29&quot;, &quot;36 to 50&quot;, &quot;51 to 64&quot;, &quot;65+&quot;), &quot;popPct&quot; = c(29, 21, 30, 20), &quot;svyPct&quot; = c(19, 21, 32, 28)) df ## age popPct svyPct ## 1 18 to 29 29 19 ## 2 36 to 50 21 21 ## 3 51 to 64 30 32 ## 4 65+ 20 28 ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) ## # A tibble: 8 × 3 ## age Group Percent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 18 to 29 Population 29 ## 2 18 to 29 Survey 19 ## 3 36 to 50 Population 21 ## 4 36 to 50 Survey 21 ## 5 51 to 64 Population 30 ## 6 51 to 64 Survey 32 ## 7 65+ Population 20 ## 8 65+ Survey 28 4.2.2 Step Two Pipe the tibble into ggplot() Specify the variable of interest with ggplot(aes(x = X)) Since we want to show the distribution of X by some group, we can use fill = to specify the group ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) 4.2.3 Step 3 Use + instead of %&gt;% to move to next line in ggplot() geom_bar() creates a barplot ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) 4.2.4 Step 4 Now, we can customize just like above with the histogram. scale_fill_grey() changes the color palette to greyscale ## Building A Basic Barplot df %&gt;% rename(Population = popPct, Survey = svyPct) %&gt;% pivot_longer(-age, names_to = &quot;Group&quot;, values_to = &quot;Percent&quot;) %&gt;% ggplot(aes(x = age, y = Percent, fill = Group)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + scale_fill_grey() + theme_minimal() + labs(x = &quot;Age Group&quot;, y = &quot;Percent&quot;, title = &quot;Population and Survey Sample Proportions by Age Group&quot;) 4.3 Now Make Your Own Histogram or Barplot! df.county %&gt;% ggplot(aes(x = minimum.wage)) + geom_histogram(aes(y = ..density..), col = &quot;pink&quot;, fill = &quot;black&quot;) + theme_economist() + labs(title = &quot;Our Beautiful Plot&quot;) "],["lab-iv-loops-lists-conditional-statements.html", "5 Lab IV: Loops, Lists, &amp; Conditional Statements 5.1 Lists 5.2 Indexing, Conditional Statements, &amp; if_else() 5.3 Loops 5.4 Lab Questions 5.5 Practice Creating Your Own Loops", " 5 Lab IV: Loops, Lists, &amp; Conditional Statements ## Packages library(tidyverse) library(mosaic) ## for the Saratoga Houses Dataset ## Data data(&quot;SaratogaHouses&quot;) 5.1 Lists 5.1.1 What is a list? A list is what is called a recursive vector A recursive vector is a vector than can contain other vectors or lists Think of lists intuitively as a more flexible vector that can contain individual vectors and even dataframes/tibbles ## Creating A List presidents &lt;- c(&quot;Washington&quot;, &quot;Adams&quot;, &quot;Jefferson&quot;, &quot;Madison&quot;, &quot;Monroe&quot;) chief_justices &lt;- c(&quot;Marshall&quot;, &quot;Warren&quot;, &quot;Burger&quot;, &quot;Rehnquist&quot;, &quot;Roberts&quot;) ages &lt;- c(51, 82, 12, 18, 43) df &lt;- data.frame(presidents, chief_justices) list_1 &lt;- list(presidents, chief_justices, ages, SaratogaHouses) ## Accessing specific objects in the list list_1[[3]] ## [1] 51 82 12 18 43 list_1[[1]] ## [1] &quot;Washington&quot; &quot;Adams&quot; &quot;Jefferson&quot; &quot;Madison&quot; &quot;Monroe&quot; 5.2 Indexing, Conditional Statements, &amp; if_else() We can pull specific values of a variable out with [] This works with vectors and dataframes/tibbles ## Pulling 4th observation of ages ages[4] ## [1] 18 ## Pulling 1st observation of presidents presidents[1] ## [1] &quot;Washington&quot; ## Pulling 2-4th observation of presidents presidents[2:4] ## [1] &quot;Adams&quot; &quot;Jefferson&quot; &quot;Madison&quot; ## Pulling all except the 3rd-5th observation of ages ages[-3] ## [1] 51 82 18 43 ## Example with a dataframe/tibble SaratogaHouses[2,4] ## [1] 22300 SaratogaHouses[1:3, 4:7] ## landValue livingArea pctCollege bedrooms ## 1 50000 906 35 2 ## 2 22300 1953 51 3 ## 3 7300 1944 51 4 You can also use indexing to pull out specific observations of a variable ## Finding the prices for all houses with 3 bathrooms SaratogaHouses$price[SaratogaHouses$bathrooms == 3] ## [1] 248800 169900 293000 205000 240000 235000 187000 293565 285558 133000 250000 322277 336000 359770 303374 500075 ## [17] 425000 465000 285000 374900 405000 278500 229000 420000 225000 349900 349900 330000 244900 253000 375000 262000 ## [33] 725000 455000 430000 397200 275055 272699 287989 300279 304065 314940 440760 255000 284595 342269 297065 345264 ## [49] 319000 307890 410000 395000 360000 281520 225570 314230 ## With dplyr SaratogaHouses %&gt;% filter(bathrooms == 3) %&gt;% pull(price) ## [1] 248800 169900 293000 205000 240000 235000 187000 293565 285558 133000 250000 322277 336000 359770 303374 500075 ## [17] 425000 465000 285000 374900 405000 278500 229000 420000 225000 349900 349900 330000 244900 253000 375000 262000 ## [33] 725000 455000 430000 397200 275055 272699 287989 300279 304065 314940 440760 255000 284595 342269 297065 345264 ## [49] 319000 307890 410000 395000 360000 281520 225570 314230 These conditional statements can get more complicated as well ## Finding the average price of a house with 5 bedrooms, 2 bathrooms, and a fireplace mean(SaratogaHouses$price[SaratogaHouses$bedrooms == 5 &amp; SaratogaHouses$bathrooms == 3 &amp; SaratogaHouses$fireplaces &gt; 0]) ## [1] 256506.7 ## With dplyr SaratogaHouses %&gt;% filter(bedrooms == 5, bathrooms == 3, fireplaces &gt; 0) %&gt;% summarise(avg_price = mean(price)) ## avg_price ## 1 256506.7 ## Finding the cheapest house with 3 bedrooms on the water min(SaratogaHouses$price[SaratogaHouses$bedrooms == 3 &amp; SaratogaHouses$waterfront == &quot;Yes&quot;]) ## [1] 319000 ## With dply SaratogaHouses %&gt;% filter(bedrooms == 3, waterfront == &quot;Yes&quot;) %&gt;% summarise(cheapest_house = min(price)) ## cheapest_house ## 1 319000 You can also use if_else() to create new variables in mutate() ## Showing if_else() SaratogaHouses %&gt;% mutate(fireplace = if_else(fireplaces &gt; 0, 1, 0)) %&gt;% select(fireplace) ## fireplace ## 1 1 ## 2 0 ## 3 1 ## 4 1 ## 5 0 ## 6 1 ## 7 1 ## 8 1 ## 9 0 ## 10 0 ## 11 0 ## 12 0 ## 13 0 ## 14 0 ## 15 0 ## 16 0 ## 17 0 ## 18 1 ## 19 0 ## 20 0 ## 21 0 ## 22 0 ## 23 0 ## 24 0 ## 25 0 ## 26 0 ## 27 0 ## 28 0 ## 29 1 ## 30 1 ## 31 1 ## 32 1 ## 33 1 ## 34 0 ## 35 1 ## 36 1 ## 37 1 ## 38 1 ## 39 1 ## 40 1 ## 41 1 ## 42 0 ## 43 1 ## 44 0 ## 45 1 ## 46 1 ## 47 0 ## 48 0 ## 49 0 ## 50 1 ## 51 0 ## 52 1 ## 53 1 ## 54 0 ## 55 1 ## 56 0 ## 57 1 ## 58 1 ## 59 0 ## 60 1 ## 61 1 ## 62 1 ## 63 0 ## 64 1 ## 65 0 ## 66 0 ## 67 1 ## 68 1 ## 69 1 ## 70 0 ## 71 0 ## 72 1 ## 73 1 ## 74 0 ## 75 0 ## 76 0 ## 77 1 ## 78 0 ## 79 0 ## 80 1 ## 81 1 ## 82 0 ## 83 0 ## 84 0 ## 85 0 ## 86 1 ## 87 0 ## 88 1 ## 89 0 ## 90 1 ## 91 0 ## 92 1 ## 93 1 ## 94 0 ## 95 1 ## 96 0 ## 97 1 ## 98 0 ## 99 0 ## 100 1 ## 101 0 ## 102 0 ## 103 1 ## 104 0 ## 105 0 ## 106 1 ## 107 1 ## 108 0 ## 109 0 ## 110 0 ## 111 1 ## 112 0 ## 113 0 ## 114 0 ## 115 1 ## 116 1 ## 117 0 ## 118 1 ## 119 0 ## 120 0 ## 121 0 ## 122 0 ## 123 0 ## 124 0 ## 125 0 ## 126 1 ## 127 1 ## 128 0 ## 129 1 ## 130 0 ## 131 0 ## 132 0 ## 133 0 ## 134 1 ## 135 0 ## 136 0 ## 137 1 ## 138 0 ## 139 1 ## 140 1 ## 141 0 ## 142 1 ## 143 1 ## 144 0 ## 145 1 ## 146 1 ## 147 0 ## 148 1 ## 149 1 ## 150 1 ## 151 1 ## 152 1 ## 153 1 ## 154 1 ## 155 1 ## 156 1 ## 157 0 ## 158 0 ## 159 0 ## 160 0 ## 161 0 ## 162 1 ## 163 1 ## 164 1 ## 165 1 ## 166 1 ## 167 1 ## 168 1 ## 169 1 ## 170 1 ## 171 1 ## 172 1 ## 173 1 ## 174 0 ## 175 1 ## 176 1 ## 177 1 ## 178 1 ## 179 1 ## 180 1 ## 181 1 ## 182 0 ## 183 0 ## 184 1 ## 185 0 ## 186 1 ## 187 1 ## 188 0 ## 189 0 ## 190 1 ## 191 1 ## 192 1 ## 193 0 ## 194 1 ## 195 1 ## 196 0 ## 197 0 ## 198 0 ## 199 1 ## 200 1 ## 201 1 ## 202 1 ## 203 1 ## 204 0 ## 205 1 ## 206 1 ## 207 1 ## 208 0 ## 209 0 ## 210 1 ## 211 1 ## 212 0 ## 213 1 ## 214 0 ## 215 0 ## 216 0 ## 217 0 ## 218 0 ## 219 0 ## 220 1 ## 221 1 ## 222 1 ## 223 0 ## 224 1 ## 225 0 ## 226 0 ## 227 0 ## 228 0 ## 229 0 ## 230 0 ## 231 1 ## 232 1 ## 233 0 ## 234 1 ## 235 0 ## 236 0 ## 237 1 ## 238 0 ## 239 0 ## 240 0 ## 241 0 ## 242 1 ## 243 0 ## 244 0 ## 245 0 ## 246 0 ## 247 0 ## 248 1 ## 249 1 ## 250 0 ## 251 1 ## 252 0 ## 253 1 ## 254 0 ## 255 1 ## 256 0 ## 257 1 ## 258 1 ## 259 1 ## 260 1 ## 261 0 ## 262 0 ## 263 1 ## 264 0 ## 265 0 ## 266 1 ## 267 0 ## 268 1 ## 269 0 ## 270 1 ## 271 0 ## 272 1 ## 273 0 ## 274 0 ## 275 1 ## 276 0 ## 277 1 ## 278 1 ## 279 1 ## 280 1 ## 281 1 ## 282 1 ## 283 1 ## 284 1 ## 285 1 ## 286 1 ## 287 1 ## 288 1 ## 289 1 ## 290 1 ## 291 1 ## 292 0 ## 293 0 ## 294 0 ## 295 1 ## 296 1 ## 297 1 ## 298 1 ## 299 1 ## 300 1 ## 301 1 ## 302 0 ## 303 1 ## 304 1 ## 305 0 ## 306 1 ## 307 0 ## 308 1 ## 309 1 ## 310 0 ## 311 1 ## 312 1 ## 313 0 ## 314 1 ## 315 1 ## 316 1 ## 317 0 ## 318 1 ## 319 1 ## 320 1 ## 321 0 ## 322 1 ## 323 1 ## 324 1 ## 325 1 ## 326 1 ## 327 0 ## 328 1 ## 329 1 ## 330 1 ## 331 1 ## 332 1 ## 333 1 ## 334 1 ## 335 1 ## 336 0 ## 337 1 ## 338 1 ## 339 1 ## 340 1 ## 341 1 ## 342 0 ## 343 1 ## 344 0 ## 345 1 ## 346 1 ## 347 1 ## 348 0 ## 349 1 ## 350 1 ## 351 1 ## 352 1 ## 353 1 ## 354 1 ## 355 0 ## 356 1 ## 357 1 ## 358 0 ## 359 1 ## 360 1 ## 361 1 ## 362 1 ## 363 1 ## 364 1 ## 365 1 ## 366 1 ## 367 0 ## 368 0 ## 369 1 ## 370 1 ## 371 1 ## 372 1 ## 373 1 ## 374 1 ## 375 0 ## 376 0 ## 377 1 ## 378 1 ## 379 1 ## 380 0 ## 381 1 ## 382 1 ## 383 1 ## 384 1 ## 385 0 ## 386 1 ## 387 1 ## 388 1 ## 389 1 ## 390 1 ## 391 0 ## 392 1 ## 393 0 ## 394 1 ## 395 1 ## 396 0 ## 397 1 ## 398 1 ## 399 1 ## 400 1 ## 401 0 ## 402 1 ## 403 1 ## 404 1 ## 405 1 ## 406 0 ## 407 1 ## 408 1 ## 409 1 ## 410 1 ## 411 0 ## 412 1 ## 413 1 ## 414 0 ## 415 1 ## 416 1 ## 417 1 ## 418 1 ## 419 1 ## 420 1 ## 421 1 ## 422 1 ## 423 0 ## 424 0 ## 425 0 ## 426 0 ## 427 0 ## 428 0 ## 429 0 ## 430 0 ## 431 1 ## 432 1 ## 433 1 ## 434 0 ## 435 1 ## 436 1 ## 437 0 ## 438 0 ## 439 1 ## 440 1 ## 441 1 ## 442 0 ## 443 0 ## 444 1 ## 445 1 ## 446 0 ## 447 0 ## 448 0 ## 449 1 ## 450 0 ## 451 0 ## 452 0 ## 453 0 ## 454 0 ## 455 0 ## 456 1 ## 457 1 ## 458 0 ## 459 0 ## 460 0 ## 461 1 ## 462 0 ## 463 0 ## 464 1 ## 465 0 ## 466 1 ## 467 1 ## 468 1 ## 469 1 ## 470 0 ## 471 1 ## 472 1 ## 473 1 ## 474 1 ## 475 0 ## 476 1 ## 477 1 ## 478 1 ## 479 1 ## 480 1 ## 481 0 ## 482 0 ## 483 1 ## 484 1 ## 485 0 ## 486 1 ## 487 1 ## 488 0 ## 489 1 ## 490 0 ## 491 1 ## 492 1 ## 493 1 ## 494 0 ## 495 0 ## 496 0 ## 497 1 ## 498 1 ## 499 1 ## 500 1 ## 501 0 ## 502 1 ## 503 1 ## 504 1 ## 505 1 ## 506 0 ## 507 1 ## 508 1 ## 509 0 ## 510 0 ## 511 0 ## 512 1 ## 513 0 ## 514 1 ## 515 1 ## 516 0 ## 517 0 ## 518 0 ## 519 1 ## 520 0 ## 521 0 ## 522 1 ## 523 0 ## 524 1 ## 525 1 ## 526 0 ## 527 1 ## 528 1 ## 529 1 ## 530 1 ## 531 1 ## 532 0 ## 533 0 ## 534 0 ## 535 1 ## 536 1 ## 537 0 ## 538 0 ## 539 1 ## 540 0 ## 541 0 ## 542 1 ## 543 0 ## 544 1 ## 545 1 ## 546 0 ## 547 0 ## 548 0 ## 549 1 ## 550 0 ## 551 1 ## 552 1 ## 553 1 ## 554 1 ## 555 0 ## 556 1 ## 557 0 ## 558 0 ## 559 1 ## 560 1 ## 561 1 ## 562 0 ## 563 1 ## 564 0 ## 565 1 ## 566 1 ## 567 1 ## 568 1 ## 569 0 ## 570 0 ## 571 0 ## 572 1 ## 573 0 ## 574 1 ## 575 1 ## 576 1 ## 577 0 ## 578 1 ## 579 1 ## 580 0 ## 581 0 ## 582 0 ## 583 1 ## 584 0 ## 585 1 ## 586 0 ## 587 0 ## 588 1 ## 589 0 ## 590 0 ## 591 1 ## 592 1 ## 593 0 ## 594 0 ## 595 1 ## 596 1 ## 597 1 ## 598 1 ## 599 0 ## 600 1 ## 601 0 ## 602 0 ## 603 1 ## 604 1 ## 605 0 ## 606 0 ## 607 1 ## 608 1 ## 609 1 ## 610 0 ## 611 0 ## 612 1 ## 613 1 ## 614 1 ## 615 1 ## 616 1 ## 617 1 ## 618 0 ## 619 0 ## 620 1 ## 621 0 ## 622 1 ## 623 1 ## 624 0 ## 625 0 ## 626 1 ## 627 0 ## 628 1 ## 629 1 ## 630 1 ## 631 0 ## 632 1 ## 633 1 ## 634 0 ## 635 0 ## 636 1 ## 637 0 ## 638 1 ## 639 1 ## 640 0 ## 641 1 ## 642 0 ## 643 1 ## 644 1 ## 645 1 ## 646 0 ## 647 0 ## 648 0 ## 649 1 ## 650 0 ## 651 0 ## 652 0 ## 653 0 ## 654 1 ## 655 1 ## 656 0 ## 657 0 ## 658 0 ## 659 1 ## 660 0 ## 661 1 ## 662 1 ## 663 1 ## 664 0 ## 665 1 ## 666 1 ## 667 0 ## 668 1 ## 669 0 ## 670 0 ## 671 0 ## 672 0 ## 673 0 ## 674 0 ## 675 1 ## 676 0 ## 677 0 ## 678 1 ## 679 0 ## 680 0 ## 681 0 ## 682 1 ## 683 1 ## 684 0 ## 685 1 ## 686 0 ## 687 1 ## 688 1 ## 689 0 ## 690 0 ## 691 0 ## 692 0 ## 693 1 ## 694 0 ## 695 1 ## 696 1 ## 697 0 ## 698 1 ## 699 1 ## 700 0 ## 701 0 ## 702 1 ## 703 1 ## 704 0 ## 705 0 ## 706 1 ## 707 0 ## 708 0 ## 709 1 ## 710 1 ## 711 1 ## 712 0 ## 713 1 ## 714 1 ## 715 1 ## 716 1 ## 717 0 ## 718 1 ## 719 0 ## 720 0 ## 721 0 ## 722 0 ## 723 0 ## 724 1 ## 725 1 ## 726 1 ## 727 0 ## 728 1 ## 729 1 ## 730 1 ## 731 0 ## 732 0 ## 733 0 ## 734 1 ## 735 1 ## 736 0 ## 737 0 ## 738 0 ## 739 0 ## 740 0 ## 741 0 ## 742 1 ## 743 0 ## 744 0 ## 745 0 ## 746 0 ## 747 0 ## 748 0 ## 749 0 ## 750 1 ## 751 0 ## 752 1 ## 753 0 ## 754 0 ## 755 0 ## 756 0 ## 757 0 ## 758 0 ## 759 0 ## 760 1 ## 761 1 ## 762 1 ## 763 1 ## 764 1 ## 765 1 ## 766 0 ## 767 0 ## 768 0 ## 769 0 ## 770 0 ## 771 0 ## 772 1 ## 773 0 ## 774 1 ## 775 0 ## 776 1 ## 777 0 ## 778 1 ## 779 1 ## 780 1 ## 781 0 ## 782 1 ## 783 0 ## 784 1 ## 785 1 ## 786 1 ## 787 0 ## 788 0 ## 789 0 ## 790 0 ## 791 0 ## 792 1 ## 793 1 ## 794 0 ## 795 1 ## 796 1 ## 797 1 ## 798 1 ## 799 0 ## 800 0 ## 801 1 ## 802 1 ## 803 1 ## 804 0 ## 805 1 ## 806 1 ## 807 1 ## 808 1 ## 809 0 ## 810 1 ## 811 1 ## 812 0 ## 813 0 ## 814 0 ## 815 1 ## 816 1 ## 817 0 ## 818 1 ## 819 0 ## 820 1 ## 821 1 ## 822 1 ## 823 1 ## 824 1 ## 825 0 ## 826 1 ## 827 1 ## 828 0 ## 829 0 ## 830 1 ## 831 0 ## 832 0 ## 833 1 ## 834 1 ## 835 1 ## 836 1 ## 837 0 ## 838 1 ## 839 1 ## 840 0 ## 841 1 ## 842 1 ## 843 1 ## 844 0 ## 845 1 ## 846 1 ## 847 1 ## 848 0 ## 849 0 ## 850 1 ## 851 1 ## 852 0 ## 853 0 ## 854 1 ## 855 0 ## 856 0 ## 857 0 ## 858 1 ## 859 0 ## 860 0 ## 861 1 ## 862 1 ## 863 1 ## 864 0 ## 865 1 ## 866 0 ## 867 1 ## 868 1 ## 869 1 ## 870 0 ## 871 1 ## 872 0 ## 873 0 ## 874 0 ## 875 0 ## 876 1 ## 877 0 ## 878 0 ## 879 0 ## 880 0 ## 881 1 ## 882 1 ## 883 1 ## 884 1 ## 885 0 ## 886 1 ## 887 0 ## 888 0 ## 889 0 ## 890 0 ## 891 1 ## 892 0 ## 893 0 ## 894 1 ## 895 0 ## 896 0 ## 897 0 ## 898 0 ## 899 1 ## 900 0 ## 901 0 ## 902 0 ## 903 1 ## 904 0 ## 905 0 ## 906 1 ## 907 1 ## 908 1 ## 909 0 ## 910 1 ## 911 0 ## 912 1 ## 913 1 ## 914 1 ## 915 0 ## 916 1 ## 917 0 ## 918 0 ## 919 0 ## 920 0 ## 921 1 ## 922 0 ## 923 0 ## 924 0 ## 925 0 ## 926 1 ## 927 1 ## 928 1 ## 929 1 ## 930 0 ## 931 1 ## 932 0 ## 933 0 ## 934 0 ## 935 0 ## 936 0 ## 937 0 ## 938 1 ## 939 1 ## 940 1 ## 941 1 ## 942 1 ## 943 1 ## 944 1 ## 945 1 ## 946 1 ## 947 1 ## 948 1 ## 949 1 ## 950 1 ## 951 1 ## 952 1 ## 953 1 ## 954 1 ## 955 1 ## 956 1 ## 957 0 ## 958 1 ## 959 1 ## 960 1 ## 961 1 ## 962 1 ## 963 1 ## 964 1 ## 965 1 ## 966 0 ## 967 0 ## 968 1 ## 969 0 ## 970 1 ## 971 1 ## 972 1 ## 973 1 ## 974 1 ## 975 1 ## 976 1 ## 977 1 ## 978 1 ## 979 0 ## 980 1 ## 981 1 ## 982 1 ## 983 1 ## 984 1 ## 985 1 ## 986 1 ## 987 1 ## 988 1 ## 989 0 ## 990 1 ## 991 1 ## 992 1 ## 993 0 ## 994 1 ## 995 0 ## 996 1 ## 997 1 ## 998 1 ## 999 0 ## 1000 0 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 728 rows ] SaratogaHouses %&gt;% mutate(large_house = if_else(rooms &gt; mean(rooms), 1, 0)) %&gt;% select(large_house) ## large_house ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 0 ## 6 1 ## 7 1 ## 8 1 ## 9 1 ## 10 0 ## 11 1 ## 12 0 ## 13 0 ## 14 0 ## 15 1 ## 16 0 ## 17 0 ## 18 1 ## 19 0 ## 20 0 ## 21 0 ## 22 0 ## 23 0 ## 24 0 ## 25 0 ## 26 1 ## 27 0 ## 28 0 ## 29 1 ## 30 0 ## 31 0 ## 32 0 ## 33 1 ## 34 0 ## 35 0 ## 36 1 ## 37 0 ## 38 1 ## 39 0 ## 40 0 ## 41 0 ## 42 1 ## 43 0 ## 44 0 ## 45 1 ## 46 1 ## 47 0 ## 48 0 ## 49 1 ## 50 0 ## 51 0 ## 52 0 ## 53 0 ## 54 0 ## 55 0 ## 56 0 ## 57 0 ## 58 0 ## 59 0 ## 60 0 ## 61 0 ## 62 1 ## 63 0 ## 64 0 ## 65 0 ## 66 1 ## 67 0 ## 68 1 ## 69 0 ## 70 0 ## 71 0 ## 72 0 ## 73 1 ## 74 0 ## 75 0 ## 76 1 ## 77 0 ## 78 0 ## 79 1 ## 80 1 ## 81 0 ## 82 0 ## 83 0 ## 84 1 ## 85 0 ## 86 0 ## 87 1 ## 88 1 ## 89 0 ## 90 1 ## 91 0 ## 92 0 ## 93 0 ## 94 0 ## 95 0 ## 96 0 ## 97 1 ## 98 0 ## 99 0 ## 100 0 ## 101 0 ## 102 0 ## 103 0 ## 104 0 ## 105 1 ## 106 0 ## 107 1 ## 108 1 ## 109 0 ## 110 0 ## 111 0 ## 112 0 ## 113 0 ## 114 0 ## 115 1 ## 116 1 ## 117 1 ## 118 0 ## 119 1 ## 120 0 ## 121 0 ## 122 0 ## 123 0 ## 124 0 ## 125 0 ## 126 0 ## 127 0 ## 128 1 ## 129 0 ## 130 0 ## 131 0 ## 132 0 ## 133 1 ## 134 1 ## 135 0 ## 136 1 ## 137 0 ## 138 0 ## 139 0 ## 140 0 ## 141 1 ## 142 0 ## 143 1 ## 144 0 ## 145 0 ## 146 1 ## 147 0 ## 148 0 ## 149 0 ## 150 0 ## 151 1 ## 152 0 ## 153 0 ## 154 0 ## 155 0 ## 156 0 ## 157 0 ## 158 0 ## 159 0 ## 160 0 ## 161 0 ## 162 1 ## 163 0 ## 164 1 ## 165 1 ## 166 0 ## 167 1 ## 168 0 ## 169 0 ## 170 1 ## 171 1 ## 172 0 ## 173 1 ## 174 0 ## 175 1 ## 176 1 ## 177 0 ## 178 1 ## 179 1 ## 180 0 ## 181 0 ## 182 0 ## 183 0 ## 184 1 ## 185 0 ## 186 0 ## 187 1 ## 188 1 ## 189 0 ## 190 0 ## 191 1 ## 192 1 ## 193 1 ## 194 0 ## 195 0 ## 196 0 ## 197 0 ## 198 1 ## 199 0 ## 200 0 ## 201 1 ## 202 0 ## 203 0 ## 204 0 ## 205 0 ## 206 1 ## 207 0 ## 208 0 ## 209 1 ## 210 0 ## 211 1 ## 212 0 ## 213 0 ## 214 1 ## 215 0 ## 216 0 ## 217 0 ## 218 0 ## 219 0 ## 220 0 ## 221 0 ## 222 1 ## 223 0 ## 224 1 ## 225 0 ## 226 0 ## 227 0 ## 228 0 ## 229 0 ## 230 1 ## 231 0 ## 232 0 ## 233 0 ## 234 0 ## 235 0 ## 236 0 ## 237 0 ## 238 0 ## 239 0 ## 240 0 ## 241 0 ## 242 0 ## 243 1 ## 244 0 ## 245 0 ## 246 0 ## 247 1 ## 248 0 ## 249 1 ## 250 0 ## 251 1 ## 252 0 ## 253 1 ## 254 0 ## 255 0 ## 256 0 ## 257 1 ## 258 0 ## 259 0 ## 260 0 ## 261 0 ## 262 1 ## 263 1 ## 264 0 ## 265 0 ## 266 0 ## 267 0 ## 268 0 ## 269 1 ## 270 1 ## 271 0 ## 272 0 ## 273 1 ## 274 1 ## 275 1 ## 276 0 ## 277 0 ## 278 1 ## 279 0 ## 280 0 ## 281 0 ## 282 1 ## 283 1 ## 284 0 ## 285 0 ## 286 1 ## 287 1 ## 288 1 ## 289 0 ## 290 1 ## 291 1 ## 292 0 ## 293 0 ## 294 0 ## 295 1 ## 296 1 ## 297 0 ## 298 1 ## 299 1 ## 300 1 ## 301 1 ## 302 0 ## 303 1 ## 304 0 ## 305 0 ## 306 0 ## 307 1 ## 308 1 ## 309 0 ## 310 1 ## 311 1 ## 312 1 ## 313 1 ## 314 1 ## 315 1 ## 316 0 ## 317 0 ## 318 0 ## 319 1 ## 320 1 ## 321 0 ## 322 1 ## 323 1 ## 324 0 ## 325 1 ## 326 0 ## 327 1 ## 328 0 ## 329 0 ## 330 1 ## 331 1 ## 332 0 ## 333 1 ## 334 1 ## 335 1 ## 336 0 ## 337 0 ## 338 1 ## 339 0 ## 340 1 ## 341 0 ## 342 0 ## 343 0 ## 344 1 ## 345 0 ## 346 1 ## 347 0 ## 348 1 ## 349 0 ## 350 1 ## 351 0 ## 352 1 ## 353 1 ## 354 0 ## 355 0 ## 356 1 ## 357 1 ## 358 0 ## 359 0 ## 360 1 ## 361 0 ## 362 1 ## 363 1 ## 364 0 ## 365 1 ## 366 1 ## 367 1 ## 368 0 ## 369 0 ## 370 1 ## 371 1 ## 372 1 ## 373 1 ## 374 0 ## 375 0 ## 376 0 ## 377 0 ## 378 0 ## 379 1 ## 380 0 ## 381 0 ## 382 1 ## 383 1 ## 384 0 ## 385 0 ## 386 1 ## 387 0 ## 388 1 ## 389 0 ## 390 1 ## 391 0 ## 392 0 ## 393 1 ## 394 0 ## 395 1 ## 396 1 ## 397 1 ## 398 0 ## 399 1 ## 400 0 ## 401 0 ## 402 0 ## 403 0 ## 404 0 ## 405 1 ## 406 0 ## 407 0 ## 408 1 ## 409 0 ## 410 1 ## 411 0 ## 412 1 ## 413 1 ## 414 0 ## 415 1 ## 416 0 ## 417 0 ## 418 1 ## 419 1 ## 420 1 ## 421 1 ## 422 0 ## 423 0 ## 424 1 ## 425 0 ## 426 1 ## 427 0 ## 428 1 ## 429 1 ## 430 0 ## 431 0 ## 432 0 ## 433 0 ## 434 1 ## 435 1 ## 436 0 ## 437 0 ## 438 1 ## 439 1 ## 440 0 ## 441 1 ## 442 0 ## 443 0 ## 444 0 ## 445 0 ## 446 0 ## 447 0 ## 448 0 ## 449 1 ## 450 0 ## 451 0 ## 452 1 ## 453 0 ## 454 0 ## 455 1 ## 456 0 ## 457 1 ## 458 1 ## 459 0 ## 460 0 ## 461 0 ## 462 1 ## 463 0 ## 464 1 ## 465 1 ## 466 0 ## 467 0 ## 468 1 ## 469 1 ## 470 1 ## 471 1 ## 472 0 ## 473 1 ## 474 0 ## 475 0 ## 476 1 ## 477 1 ## 478 1 ## 479 0 ## 480 0 ## 481 0 ## 482 1 ## 483 1 ## 484 0 ## 485 0 ## 486 1 ## 487 0 ## 488 1 ## 489 1 ## 490 0 ## 491 1 ## 492 1 ## 493 1 ## 494 1 ## 495 0 ## 496 0 ## 497 0 ## 498 0 ## 499 1 ## 500 0 ## 501 0 ## 502 1 ## 503 1 ## 504 1 ## 505 1 ## 506 1 ## 507 1 ## 508 0 ## 509 0 ## 510 0 ## 511 1 ## 512 0 ## 513 0 ## 514 0 ## 515 0 ## 516 0 ## 517 0 ## 518 1 ## 519 0 ## 520 0 ## 521 0 ## 522 1 ## 523 0 ## 524 1 ## 525 0 ## 526 0 ## 527 0 ## 528 1 ## 529 0 ## 530 0 ## 531 1 ## 532 0 ## 533 0 ## 534 0 ## 535 1 ## 536 1 ## 537 1 ## 538 1 ## 539 1 ## 540 0 ## 541 0 ## 542 0 ## 543 0 ## 544 0 ## 545 0 ## 546 0 ## 547 1 ## 548 1 ## 549 1 ## 550 0 ## 551 0 ## 552 0 ## 553 1 ## 554 1 ## 555 1 ## 556 0 ## 557 0 ## 558 0 ## 559 0 ## 560 0 ## 561 0 ## 562 0 ## 563 0 ## 564 0 ## 565 0 ## 566 1 ## 567 1 ## 568 1 ## 569 1 ## 570 0 ## 571 1 ## 572 1 ## 573 1 ## 574 1 ## 575 1 ## 576 0 ## 577 0 ## 578 0 ## 579 1 ## 580 0 ## 581 0 ## 582 1 ## 583 1 ## 584 0 ## 585 1 ## 586 0 ## 587 0 ## 588 0 ## 589 0 ## 590 1 ## 591 1 ## 592 1 ## 593 0 ## 594 1 ## 595 1 ## 596 0 ## 597 1 ## 598 1 ## 599 0 ## 600 1 ## 601 1 ## 602 1 ## 603 0 ## 604 1 ## 605 0 ## 606 1 ## 607 1 ## 608 0 ## 609 0 ## 610 0 ## 611 0 ## 612 1 ## 613 0 ## 614 1 ## 615 0 ## 616 1 ## 617 0 ## 618 1 ## 619 0 ## 620 0 ## 621 0 ## 622 1 ## 623 0 ## 624 1 ## 625 0 ## 626 1 ## 627 0 ## 628 0 ## 629 1 ## 630 0 ## 631 0 ## 632 1 ## 633 1 ## 634 1 ## 635 0 ## 636 0 ## 637 0 ## 638 0 ## 639 0 ## 640 0 ## 641 0 ## 642 0 ## 643 0 ## 644 1 ## 645 0 ## 646 1 ## 647 1 ## 648 0 ## 649 1 ## 650 1 ## 651 0 ## 652 0 ## 653 0 ## 654 1 ## 655 1 ## 656 0 ## 657 0 ## 658 0 ## 659 0 ## 660 0 ## 661 0 ## 662 1 ## 663 0 ## 664 0 ## 665 0 ## 666 1 ## 667 0 ## 668 0 ## 669 0 ## 670 0 ## 671 0 ## 672 1 ## 673 0 ## 674 0 ## 675 0 ## 676 0 ## 677 0 ## 678 1 ## 679 1 ## 680 0 ## 681 0 ## 682 0 ## 683 1 ## 684 1 ## 685 1 ## 686 1 ## 687 0 ## 688 0 ## 689 1 ## 690 0 ## 691 0 ## 692 1 ## 693 0 ## 694 0 ## 695 0 ## 696 0 ## 697 1 ## 698 0 ## 699 0 ## 700 0 ## 701 1 ## 702 1 ## 703 0 ## 704 1 ## 705 0 ## 706 1 ## 707 1 ## 708 1 ## 709 1 ## 710 0 ## 711 1 ## 712 0 ## 713 0 ## 714 1 ## 715 0 ## 716 1 ## 717 0 ## 718 0 ## 719 0 ## 720 1 ## 721 0 ## 722 0 ## 723 1 ## 724 1 ## 725 0 ## 726 1 ## 727 0 ## 728 1 ## 729 1 ## 730 1 ## 731 0 ## 732 1 ## 733 0 ## 734 1 ## 735 1 ## 736 0 ## 737 0 ## 738 1 ## 739 1 ## 740 0 ## 741 0 ## 742 0 ## 743 0 ## 744 1 ## 745 0 ## 746 0 ## 747 0 ## 748 0 ## 749 0 ## 750 0 ## 751 0 ## 752 1 ## 753 0 ## 754 0 ## 755 0 ## 756 0 ## 757 1 ## 758 1 ## 759 1 ## 760 1 ## 761 0 ## 762 1 ## 763 0 ## 764 0 ## 765 0 ## 766 1 ## 767 0 ## 768 1 ## 769 0 ## 770 0 ## 771 0 ## 772 0 ## 773 0 ## 774 1 ## 775 0 ## 776 0 ## 777 0 ## 778 1 ## 779 0 ## 780 1 ## 781 0 ## 782 0 ## 783 0 ## 784 0 ## 785 0 ## 786 1 ## 787 1 ## 788 0 ## 789 0 ## 790 0 ## 791 0 ## 792 0 ## 793 1 ## 794 1 ## 795 0 ## 796 1 ## 797 1 ## 798 1 ## 799 1 ## 800 0 ## 801 1 ## 802 1 ## 803 1 ## 804 0 ## 805 0 ## 806 0 ## 807 1 ## 808 1 ## 809 0 ## 810 0 ## 811 1 ## 812 1 ## 813 0 ## 814 1 ## 815 1 ## 816 0 ## 817 0 ## 818 1 ## 819 1 ## 820 0 ## 821 0 ## 822 1 ## 823 1 ## 824 1 ## 825 0 ## 826 1 ## 827 1 ## 828 0 ## 829 1 ## 830 0 ## 831 0 ## 832 1 ## 833 1 ## 834 0 ## 835 1 ## 836 1 ## 837 0 ## 838 1 ## 839 0 ## 840 0 ## 841 1 ## 842 0 ## 843 0 ## 844 0 ## 845 0 ## 846 1 ## 847 0 ## 848 0 ## 849 0 ## 850 1 ## 851 0 ## 852 0 ## 853 1 ## 854 0 ## 855 0 ## 856 0 ## 857 0 ## 858 0 ## 859 0 ## 860 0 ## 861 0 ## 862 0 ## 863 1 ## 864 1 ## 865 1 ## 866 0 ## 867 1 ## 868 0 ## 869 1 ## 870 0 ## 871 0 ## 872 0 ## 873 0 ## 874 0 ## 875 0 ## 876 0 ## 877 1 ## 878 0 ## 879 0 ## 880 0 ## 881 0 ## 882 0 ## 883 0 ## 884 1 ## 885 0 ## 886 1 ## 887 0 ## 888 0 ## 889 0 ## 890 0 ## 891 0 ## 892 0 ## 893 0 ## 894 1 ## 895 0 ## 896 0 ## 897 0 ## 898 0 ## 899 0 ## 900 0 ## 901 0 ## 902 0 ## 903 0 ## 904 0 ## 905 0 ## 906 0 ## 907 0 ## 908 1 ## 909 1 ## 910 0 ## 911 0 ## 912 1 ## 913 0 ## 914 0 ## 915 0 ## 916 1 ## 917 0 ## 918 0 ## 919 0 ## 920 0 ## 921 0 ## 922 0 ## 923 0 ## 924 0 ## 925 1 ## 926 1 ## 927 0 ## 928 0 ## 929 1 ## 930 1 ## 931 1 ## 932 0 ## 933 0 ## 934 0 ## 935 0 ## 936 0 ## 937 0 ## 938 0 ## 939 0 ## 940 1 ## 941 1 ## 942 0 ## 943 0 ## 944 1 ## 945 1 ## 946 0 ## 947 0 ## 948 0 ## 949 0 ## 950 0 ## 951 1 ## 952 1 ## 953 1 ## 954 1 ## 955 1 ## 956 1 ## 957 0 ## 958 1 ## 959 1 ## 960 1 ## 961 1 ## 962 1 ## 963 0 ## 964 1 ## 965 1 ## 966 1 ## 967 0 ## 968 0 ## 969 0 ## 970 1 ## 971 0 ## 972 1 ## 973 0 ## 974 0 ## 975 1 ## 976 0 ## 977 1 ## 978 0 ## 979 0 ## 980 1 ## 981 1 ## 982 1 ## 983 0 ## 984 0 ## 985 1 ## 986 0 ## 987 1 ## 988 0 ## 989 0 ## 990 0 ## 991 1 ## 992 1 ## 993 0 ## 994 0 ## 995 0 ## 996 0 ## 997 1 ## 998 0 ## 999 1 ## 1000 0 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 728 rows ] 5.3 Loops 5.3.1 What is a Loop? A central concept of programming that is found in most programming languages Loops are control statements that execute one or more statements for a desired number of times Loops can be used to iterate applying a function a certain number of times to a specified object(s) 5.3.2 How Loops Work in R ## Basic Loop for (i in 1:5) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Without print() What happened? ## Basic Loop for (i in 1:5) { i } Works for character and numeric vectors too ## Character Vector parties &lt;- c(&quot;Democratic&quot;, &quot;Republican&quot;, &quot;Libertarian&quot;, &quot;Green&quot;) for (i in parties) { print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Republican&quot; ## [1] &quot;Libertarian&quot; ## [1] &quot;Green&quot; ## Numeric Vector numbers &lt;- c(1, 2, 3, 4, 5) for (i in numbers) { print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Let’s write a loop that applies the square root function to a vector of integers ## Square Root Loop for (i in 1:length(numbers)) { print(sqrt(i)) } ## [1] 1 ## [1] 1.414214 ## [1] 1.732051 ## [1] 2 ## [1] 2.236068 5.3.3 Conditional Statements &amp; Stopping Loops ## A Loop That Stops for (i in parties) { if (i == &quot;Libertarian&quot;) { break } print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Republican&quot; 5.3.4 Conditional Statements &amp; Skipping Iterations ## A Loop That Skips An Iteration for (i in parties) { if (i == &quot;Republican&quot;) { next } print(i) } ## [1] &quot;Democratic&quot; ## [1] &quot;Libertarian&quot; ## [1] &quot;Green&quot; 5.3.5 if else Statements ## Using if else for (i in numbers) { if (i &gt; 3) { print(&quot;Number Greater Than 3&quot;) } else { print(&quot;Number Less Than 4&quot;) } print(i) } ## [1] &quot;Number Less Than 4&quot; ## [1] 1 ## [1] &quot;Number Less Than 4&quot; ## [1] 2 ## [1] &quot;Number Less Than 4&quot; ## [1] 3 ## [1] &quot;Number Greater Than 3&quot; ## [1] 4 ## [1] &quot;Number Greater Than 3&quot; ## [1] 5 5.3.6 More Complicated Loops What is going on here? page 146 in Imai &amp; Williams (2022) ## Example from QSS values &lt;- c(2, 4, 6) n &lt;- length(values) results &lt;- rep(NA, n) for (i in seq_along(values)) { results[i] &lt;- values[i] * 2 print(str_c(values[i], &quot; times 2 is equal to &quot;, results[i])) } ## [1] &quot;2 times 2 is equal to 4&quot; ## [1] &quot;4 times 2 is equal to 8&quot; ## [1] &quot;6 times 2 is equal to 12&quot; What is going on here? ## Loop to Calculate A Series of Means for (i in 1:length(unique(SaratogaHouses$bedrooms))) { x &lt;- mean(SaratogaHouses$price[SaratogaHouses$bedrooms == i]) names(x) &lt;- i print(x) } ## 1 ## 192771.4 ## 2 ## 152561.3 ## 3 ## 200678 ## 4 ## 265550.6 ## 5 ## 276577.5 ## 6 ## 277328.8 ## 7 ## 226666.7 Same thing with library(tidyverse) ## Using dplyr() SaratogaHouses %&gt;% group_by(bedrooms) %&gt;% summarise(avg_price = mean(price)) ## # A tibble: 7 × 2 ## bedrooms avg_price ## &lt;int&gt; &lt;dbl&gt; ## 1 1 192771. ## 2 2 152561. ## 3 3 200678. ## 4 4 265551. ## 5 5 276578. ## 6 6 277329. ## 7 7 226667. 5.3.7 Conditional Means A conditional mean is simply the mean of some variable given when a certain set of conditions are met. We do this in R by indexing and subsetting. As an example, assume that you may be interested in voter turnout by identity group. Thus, you are calculating the mean of voter turnout conditional on identity status. Remember this for regression to help intuitively understand what OLS is doing. 5.3.8 Factor Variables A factor variable is a categorical variable that can only take a distinct set of values. An example is marital status which could take single, married, or divorced. A categorical variable like a people’s names is not a factor variable as it could essentially take an infinite number of possible values. 5.4 Lab Questions 5.4.1 Use a loop and library(dplyr) to calculate the maximum price of a house conditional on the number of rooms that a house has. ## Loop for (i in 1:length(unique(SaratogaHouses$rooms))) { x &lt;- mean(SaratogaHouses$price[SaratogaHouses$rooms == i], na.rm = T) names(x) &lt;- i print(x) } ## 1 ## NaN ## 2 ## 94500 ## 3 ## 134156.2 ## 4 ## 168917.7 ## 5 ## 167297.6 ## 6 ## 185313.7 ## 7 ## 191829 ## 8 ## 220596.8 ## 9 ## 245278.8 ## 10 ## 288567.2 ## 11 ## 305913.7 ## With dplyr SaratogaHouses %&gt;% group_by(rooms) %&gt;% summarise(avg_price = mean(price)) ## # A tibble: 11 × 2 ## rooms avg_price ## &lt;int&gt; &lt;dbl&gt; ## 1 2 94500 ## 2 3 134156. ## 3 4 168918. ## 4 5 167298. ## 5 6 185314. ## 6 7 191829. ## 7 8 220597. ## 8 9 245279. ## 9 10 288567. ## 10 11 305914. ## 11 12 373219. 5.4.2 Create a loop that calculates the maximum and minimum house prices by number of bedrooms. Replicate with library(dplyr) ## With Loop for (i in 1:length(unique(SaratogaHouses$bedrooms))) { max_price &lt;- max(SaratogaHouses$price[SaratogaHouses$bedrooms == i], na.rm = T) min_price &lt;- min(SaratogaHouses$price[SaratogaHouses$bedrooms == i], na.rm = T) names(max_price) &lt;- i names(min_price) &lt;- i print(c(max_price, min_price)) } ## 1 1 ## 315000 78000 ## 2 2 ## 655000 10300 ## 3 3 ## 775000 5000 ## 4 4 ## 725000 65000 ## 5 5 ## 775000 119900 ## 6 6 ## 422680 95000 ## 7 7 ## 325000 131000 ## With dplyr SaratogaHouses %&gt;% group_by(bedrooms) %&gt;% summarise(max_price = max(price), min_price = min(price)) ## # A tibble: 7 × 3 ## bedrooms max_price min_price ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 315000 78000 ## 2 2 655000 10300 ## 3 3 775000 5000 ## 4 4 725000 65000 ## 5 5 775000 119900 ## 6 6 422680 95000 ## 7 7 325000 131000 5.4.3 What is the most number of fireplaces in a house with five bedrooms? ## Most Fireplace in 5 bedroom house max(SaratogaHouses$fireplaces[SaratogaHouses$bedrooms == 5]) ## [1] 4 5.5 Practice Creating Your Own Loops Use SaratogaHouses or use the data() command to bring in a dataset of your interest and write a loop to carry out an easy task. "]]

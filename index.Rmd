---
title: 'GOVT 8001: Shared Working Book'
author: "Benjamin Reese"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

## Packages
library(tidyverse)
library(qss)

```

# Introduction

```{r, echo=FALSE}
knitr::include_graphics("intro_image_.jpg")
```

## Purpose of This Site

The goal of this site is to have a place where you can look for examples of code. Think of this site as a log of all of the topics we cover in class and in lab sessions. I will make updates throughout the semester as we cover more advanced material.

Let me know if any of this code doesnâ€™t run correctly or if you have any questions or issues!

## Loading Data

The easiest way to load data into R and ensure you have the correct file path is to create a folder on your computer for each assignment and place the datasets directly into that folder.

1. Create a folder on your computer for each new analysis

2. Download your Data and move the file to your newly created folder

3. Then open RStudio

4. Click the project button in the top right corner

5. Click new project

6. Click existing directory

7. Click browse and find the folder that you created

8. Click create project

9. Once your new project opens, click the blank page with a green plus sign icon in the top left corner under the file option

10. Click R script to open a new script

11. You should also be able to see your data file in the bottom right window of RStudio, click the file and follow the options depending on the file type

12. Once your data is imported into R, the code that R automatically ran will be in the console window on the bottom left, copy and paste it to your fresh R script
  - For example, in Lab 1, my code looked like: `read_excel("Data/USstates.xlsx")`

13. Run this copy and pasted line of code whenever you open the R Project and you will never have to worry about complicated file pathing commands

14. I recommend using the assignment operator `<-` to give  your dataset a short and simple name like `df`, `dta`, or, if you are working with multiple datasets, name each something short and descriptive.

# Lab I: Introduction to R & R Studio

```{r, echo=FALSE}
knitr::include_graphics("image_1.png")
```

## Intro to R

```{r}
# Author: QSS Ch. 1 script with edits by Mark Richardson & Benjamin Reese
# Date: 08/24/2023
# Purpose: Introduction to R - GOVT 8001 Lab I

#### Arithmetic Operations ####

5 + 3
5 - 3
5 / 3
5 ^ 3
5 * (10 - 3)
sqrt(4)

#### Objects ####

result <- 5 + 3
result
print(result)
result <- 5 - 3
result
```


```{r, eval=FALSE}
## R is case sensitive so we get an error.
Result
```


```{r}
kosuke <- "instructor"
kosuke
kosuke <- "instructor and author"
kosuke

Result <- 5
Result + 2
result

class(result)
Result
class(Result)
class(sqrt)

sum(result)

sum(Result)

#### Vectors ####

# Creating vectors

world.pop <- c(2525779, 3026003, 3691173, 4449049, 5320817, 6127700, 6916183)
world.pop

pop.first <- c(2525779, 3026003, 3691173)
pop.second <- c(4449049, 5320817, 6127700, 6916183)
pop.all <- c(pop.first, pop.second)
pop.all

# Accessing elements of a vector

world.pop[2]
world.pop[c(2, 4)] 
world.pop[c(4, 2)] 
world.pop[-3]

# Arithmetic operations on a vector

pop.million <- world.pop / 1000
pop.million

pop.rate <- world.pop / world.pop[1]
pop.rate

pop.increase <- world.pop[-1] - world.pop[-7]
pop.increase

percent.increase <- (pop.increase / world.pop[-7]) * 100
percent.increase

# Can replace individual elements (better way is to use round())

round(percent.increase)

percent.increase[c(1, 2)] <- c(20, 22)
percent.increase

#### Functions ####

length(world.pop)  

min(world.pop)     

max(world.pop)     

range(world.pop)   

mean(world.pop)    

sum(world.pop) / length(world.pop) 

year <- seq(from = 1950, to = 2010, by = 10)
year

seq(to = 2010, by = 10, from = 1950)

seq(1950, 2010, 10)

seq(2010, 1950, -10)

seq(from = 2010, to = 1950, by = -10)
2008:2012
2012:2008

names(world.pop) 
names(world.pop) <- year
names(world.pop)

world.pop

#### Saving data and loading data ####
 
# Create a data set (Table 1.2)
# tibble() is the equivalent of data.frame() tidyverse function from the tibble package
UNpop <- data.frame(world.pop = world.pop,
                    year = year)

# Get basic information about the data set

names(UNpop)

nrow(UNpop)

ncol(UNpop)

dim(UNpop)

summary(UNpop)

UNpop$world.pop

UNpop[, "world.pop"] # extract the column called "world.pop"
UNpop[c(1, 2, 3, 5), ]   # extract the first three rows (and all columns)
UNpop[1:3, "year"]   # extract the first three rows of the "year" column

UNpop$world.pop[seq(from = 1, to = nrow(UNpop), by = 2)]

# File paths and working directory

getwd() # Confirm the change

#### Getting Help: mean() example ####

world.pop <- c(UNpop$world.pop, NA)
world.pop

mean(world.pop)

## Use Question Marks to see help documentation
?mean

mean(world.pop, na.rm = TRUE)
```

## Intro to `library(tidyverse)`

```{r}

# Packages

## install.packages("devtools") # install the package
library(devtools) # load the package

## install a package from github
## devtools::install_github("kosukeimai/qss-package", build_vignettes = TRUE)
library(qss) ## loading in qss
## You may need to allow R to update/install additional packages

## Loading in tidyverse
## install.packages("tidyverse")
library(tidyverse)

## Loading in a Dataset
data(UNpop, package = "qss")

## Number of Rows and Columns - Base R
dim(UNpop)

## Number of observation, number of variables, and initial observations - tidyverse
glimpse(UNpop)

## First 6 rows
head(UNpop)

## Last 6 Rows
tail(UNpop)

## Selecting A Variable - Base R
UNpop$world.pop

## subset all rows for the column called "world.pop" from the UNpop data
UNpop[, "world.pop"]
## subset the first three rows (and all columns)
UNpop[c(1, 2, 3),]
## subset the first three rows of the "year" column
UNpop[1:3, "year"]

## Now with tidyverse

## Subset the first three rows of UNpop with tidyverse
slice(UNpop, 1:3)

## Extract/subset the world.pop variable (column)
select(UNpop, world.pop)

## Base R subset the first three rows of the year variable
UNpop[1:3, "year"]
## or in tidyverse, combining slice() and select()
select(slice(UNpop, 1:3), year)

## Basic Data Wrangling with the tidyverse using pipes (i.e., %>%)

UNpop %>% # take the UNpop data we have loaded, and then...
  slice(1:3) %>% # subset the first three rows, and then...
  select(year) # subset the year column

UNpop %>%
  slice(seq(1, n(), by = 2)) %>% # using a sequence from 1 to n()
  select(world.pop)

pop.1970 <- UNpop %>% # take the UNpop data and then....
  filter(year == 1970) %>% # subset rows where the year variable is equal to 1970
  select(world.pop) %>% # subset just the world.pop column
  pull() # return a vector, not a tibble

## Print the vector to the console to see it
print(pop.1970)

UNpop.mill <- UNpop %>% # create a new tibble from UNpop
  mutate(world.pop.mill = world.pop / 1000) %>% # create a new variable, world.pop.mill
  select(-world.pop) # drop the original world.pop column

## Adding a variable with if_else
UNpop.mill <- UNpop.mill %>%
  mutate(after.1980 = if_else(year >= 1980, 1, 0))

## Creating a vector of the years of interest
specific.years <- c(1950, 1980, 2000)

## Adding a variable with if_else and %in%
UNpop.mill <- UNpop.mill %>%
  mutate(year.of.interest = if_else(year %in% specific.years, 1, 0))

summary(UNpop.mill)
mean(UNpop.mill$world.pop.mill)

## Add a row where values for all columns is NA
UNpop.mill.wNAs <- UNpop.mill %>%
  add_row(year = NA, world.pop.mill = NA,
          after.1980 = NA,
          year.of.interest = NA)
## Take the mean of world.pop.mill (returns NA)
mean(UNpop.mill.wNAs$world.pop.mill)
## Take the mean of world.pop.mill (ignores the NA)
mean(UNpop.mill.wNAs$world.pop.mill, na.rm = TRUE)

## Other Summary Statistics with tidyverse
UNpop.mill %>%
  summarize(mean.pop = mean(world.pop.mill),
            median.pop = median(world.pop.mill))

UNpop.mill %>%
  group_by(after.1980) %>% # create subset group for each value of after.1980
  summarize(mean.pop = mean(world.pop.mill)) # calculate mean for each group

```

# Lab II: Introduction to `library(tidyverse)` & R Markdown

```{r, echo=FALSE}
knitr::include_graphics("image_2_.png")
```


We can use R Markdown to create well-formatted PDFs or .html files that can easily display the results of our analyses. R Markdown, through Latex, also allows to write mathematical formulas with ease. Go ahead and knit - found in the top left corner - this file now and see what it looks like.

## In the setup chunk above, load the tidyverse packages as well as library(readr)

```{r, echo=TRUE, eval=FALSE}
## Example Setup Chunk

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
## Packages
library(readr)
library(tidyverse)

```

## Load in the resume.RData file and use head(), tail(), glimpse(), dim(), summary(), and View() to examine each variable in the dataset. How many of the resumes have white sounding names? How many have African-American sounding names.

```{r}
## Loading Data
data(resume, package = "qss")

## Learning About the Dataset

head(resume)

tail(resume)

glimpse(resume)

dim(resume)

summary(resume)

View(resume) ## Comment this out when knitting.

## Number of observations by race
resume %>%
  group_by(race) %>%
  count()

```

## This experiment seeks to determine whether or not hiring managers discriminate on the basis of racial identity by sending idential resumes with African-American and white sounding names to job postings. The basic logic is that resumes are identical and only the name is changing, so any differences in call backs for jobs can be attributed to racial discrimination. Why do the authors want to randomize? And do you think this is an effective research design?

The concerns of examining race and the number of callbacks without randomization is that there could be confounders like workplace connections, amount of education, and employment history that could be correlated with race. It is possible that African-American applicants did not have the same employment and educational opportunities as white Americans, and, therefore, their resumes may look significantly different. This raises issues of counfounding and makes it impossible to differentiate if an employer made their decision based on race or based on the substance of the resume. The authors, though, randomized race, reducing this risk of confounding. Employers in this study are seeing nearly identical resumes, with only the race of the applicant being different, as indicated by a name. 

The field experiment presented here relies on racial connotations of different names, not explicit racial cues. Therefore, hiring managers are determining an applicant's race largely based on what scholars of identity politics would call "perceived race" or "street race" ([Lopez et al. 2017](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5800755/)), which is how others perceive an individual's race. This fact means that the selection of names is integral to the internal validity of the research design.

## We are going to see if there is a racial discrepency by taking the difference in callback rates between racial groups. Calculate the callback rate for white sounding name applicants and African-American sounding name applicants. Use Latex commands to write the formula for this calculation and display the result in text. Write the formula between $'s like $y = mx + b$ to use Latex commands.

```{r}
## Call Back for white Sounding Name Applicants
resume %>%
  group_by(race) %>%
  summarise(callback_rates = mean(call))

```

The callback rate for whites is .096. We take the mean of the binary callback variable, $\overline{x} = \frac{1}{n}\Sigma^{n}_{i=1}x_i$

The callback rate for African-American sounding name applicants is .064.

## Now, create a new object that stores the difference in callback rates named race_diff.

```{r}
## Calculating Callback Proportions
race_call <- resume %>%
  group_by(race, call) %>%
  count() %>%
  pivot_wider(names_from = call,
              values_from = n) %>%
  rename(no_call = `0`,
         call = `1`) %>%
  mutate(total_resumes = no_call + call,
         call_prop = call / total_resumes)

## Difference in call back rates
race_diff <- race_call %>% 
  select(race, call_prop) %>%
  pivot_wider(names_from = c(race),
              values_from = call_prop) %>%
  mutate(race_diff = white - black) %>%
  select(race_diff)

## Printing
race_diff

```

## Since Crenshaw (1989), manny scholars have concerned with intersectionality, or how race and gender interact to make the experiences of African-American women unique. We can use the data we have to explore the effect of race and gender specific sounding names on employment prospects. Calculate the call back rate by each race and gender category.

```{r}
## Callbacks by race and gender
resume %>%
  group_by(race, call, sex) %>%
  count() %>%
  pivot_wider(names_from = call,
              values_from = n) %>%
  rename(no_call = `0`,
         call = `1`) %>%
  mutate(total_resumes = no_call + call,
         call_prop = call / total_resumes)

```

## What is the difference in call back rates for each race/gender group?

```{r}
## Saving tibble from 8
dta <- resume %>%
  group_by(race, call, sex) %>%
  count() %>%
  pivot_wider(names_from = call,
              values_from = n) %>%
  rename(no_call = `0`,
         call = `1`) %>%
  mutate(total_resumes = no_call + call,
         call_prop = call / total_resumes)

## Calculating Differences
call_backs <- dta %>% 
  select(race, sex, call_prop) %>%
  pivot_wider(names_from = c(sex, race),
              values_from = call_prop) %>%
  mutate(white_sex_diff = female_white - male_white,
         black_sex_diff = female_black - male_black,
         male_race_diff = male_white - male_black,
         female_race_diff = female_white - female_black) %>%
  select(white_sex_diff, black_sex_diff, male_race_diff, female_race_diff)

## Printing
print(call_backs) ## print() is optional

```